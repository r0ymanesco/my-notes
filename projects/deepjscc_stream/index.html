<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeepJSCC Stream | My Notebook</title><meta name=keywords content="Projects"><meta name=description content="Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:
The implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train."><meta name=author content="Tze-Yang Tung"><link rel=canonical href=https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/><link crossorigin=anonymous href=/my-notes/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-notes/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://r0ymanesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://r0ymanesco.github.io/my-notes/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://r0ymanesco.github.io/my-notes/favicon-32x32.png><link rel=apple-touch-icon href=https://r0ymanesco.github.io/my-notes/apple-touch-icon.png><link rel=mask-icon href=https://r0ymanesco.github.io/my-notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><meta property="og:title" content="DeepJSCC Stream"><meta property="og:description" content="Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:
The implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train."><meta property="og:type" content="article"><meta property="og:url" content="https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-01-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-04T10:29:08+00:00"><meta property="og:site_name" content="My Notebook"><meta name=twitter:card content="summary"><meta name=twitter:title content="DeepJSCC Stream"><meta name=twitter:description content="Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:
The implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Projects","item":"https://r0ymanesco.github.io/my-notes/projects/"},{"@type":"ListItem","position":3,"name":"DeepJSCC Stream","item":"https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeepJSCC Stream","name":"DeepJSCC Stream","description":"Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:\nThe implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train.","keywords":["Projects"],"articleBody":"Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:\nThe implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train. It would make more sense to try to improve the current implementation.\nNew encode/decode scheme What transformers are really good at is autoregression. So the question we want to pose to a transformer is: “given the prior tokens, what is the next token?”\nGiven frames \\( \\{x_1, …, x_n\\} \\), with corresponding features \\( y_i = E(x_i) \\).\nEncoder: learn a “query” token \\( Q_z \\) that learns the codeword given a sequence of frame features \\( \\{y_1,…,y_n\\} \\). That is, we are modelling \\( P(z_n | y_1, …, y_n) \\) by querying the self-attention output of \\( \\{y_1,…,y_n\\} \\). This can be done sequentially as frames arrive and we can observe the entropy of \\( z_n \\) to determine the GoP cut-off. Causality is naturally preserved. Note that this is NOT an autoregressive process. If we want the codeword to be autoregressive, we can condition on \\( (y_i, z_{n-1}) \\). Decoder: given \\( z_n \\), learn a “start decode” token \\( \\hat{y}_0 \\), such that we autoregressively decode the frame features. That is, we are modelling \\( P(\\hat{y}_n | \\hat{y}_0,…, \\hat{y}_{n-1}; z_n) \\). The estimated frame can then be obtained as \\( \\hat{x}_n = D(\\hat{y}_n) \\). To ensure causality, the decoding should be masked. Unfortunately the new design still does not work. The next step is to try an autoregressive encoder. The encoding will be slower due to the iterative nature of the code but it may be better due to the progressiveness rather than single step coding.\nAnother option is to implement the same idea but using 2D transformers. This is easier so will go first.\nConnection to R-D Given a channel bandwidth constraint, you could in theory traverse the R-D trade-off by considering, given a target distortion, how many frames can I encode into the available channel resources? The distortion would be lower-bounded by the distortion offered by encoding 1 frame. More frames encoded implies lower rate (per frame) and therefore higher distortion. This is in contrast to typical R-D trade-off, as this implies that greater latency (encoding more frames) increases distortion.\nSnapshot compressive imaging (SCI) The STformer seems to be a good way to perform 2D transformer. Moreover, the snapshot compressive imaging (SCI) problem, sounds very interesting and may be applicable to our problem. In the SCI formulation, the encoding is done via the summation of frames, each multiplied with a mask. Then given the mask, and the result of the summation, the decoder tries to invert the process. This is akin to an under-determined inverse problem. If we use this for our coding problem, this would essentially entail a simple encoder, where each frame is mapped to a feature and those features are summed together to form the codeword. Then the decoder, knowing the filters used in the encoding process, inverts the summation and reproduces the original frames.\nSCI is basically an inverse problem (very similar to compressive sensing). We can use a transformer to learn the masks for each feature tensor and sum them up. The receiver then computes the inverse problem given the “measurements” (masks). If we want to achieve autoregressive coding, we can just condition the next mask on the previous masks. The decoder can be like the STformer This idea has been shelved for now as it requires the masks to be sent separately.\nVCT based R-D prediction We can use the same model as VCT to predict the distribution and use it to determine the number of channel uses. Then a second transformer would predict the image/video quality instead of the distribution. Once both transformers are trained, we can traverse the R-D curve freely. That is, we first map frames into the latent space via an encoder \\( z_t = E(x_t) \\). Then, we want to estimate \\( Q(z_t) = f_\\theta(z_{t-1},…,z_0) \\), where \\( Q \\) is the quality function, parameterised by \\( \\theta \\).\nTo generate codewords, we condition the current frame’s latent vector on the previous frames’ latent vectors \\( c_t = f(z_t | z_{t-1},…,z_0) \\), using a transformer.\nCurrently, the coding stage uses reconstructions from the receiver at a random rate to infer the current frame. This might lead to suboptimal results as initial reconstructions are poor, therefore affecting the reconstructions of future frames. However, accumulating gradients across the entire video sequence would require too much memory. Perhaps there is a way to do curriculum training, starting with feature encoder only before jointly training with the transformer, to find a middle ground.\nIn order to preserve the ordering of rate-distortion curve, the output of the predictor is a cumulative sum. The predictor training using a transformer conditioned on prior frames does indeed work. The question is whether the accuracy is good enough to determine the rate. This result is also based on codewords which are the output of the feature encoder, without prior frame conditioning.\nCompression quality prediction The same concept can be used to predict the compression quality for a given number of bits. The VCT paper only considers average quality for a given number of bits. If we allow variable rate compression, then we can instead target a fixed quality and adjust the bit-rate to obtain it. The result would be for a chosen distortion, an average bit-rate to achieve it.\nArchitecture According to the VCT video compression paper, it is possible to train jointly with this architecture using the architecture in ELIC . Preliminary results seems to suggest a pretty substantial improvement using this architecture, over the one proposed by VCT. The primary difference is that the GDN layers are replaced by residual layers. It is interesting to see that the GDN layer is becoming out of fashion.\nOn the other hand, the latent residual predictor proposed by the VCT paper was not useful for our case since the codeword and the decoder both already condition on the previous frames as well as the current frame tokens.\nHyperparameters In the ELIC paper, they suggest using dynamic weights on the loss of different rates to improve the generalisation for each rate. There are two approaches:\nThe idea is that, let \\( D_r \\) be the average distortion of the model at rate \\( r \\), then if the average improves by more than a constant factor \\( \\delta \\), then the weighting \\( \\mu_r \\) at that rate will decrease, and vice versa. The loss is then \\( \\sum_r \\mu_r D_r \\). Another approach is to compare frames at successive rates and ensure that the improvement gap between each successive rate is sufficiently large. The paper actually suggests using a different rate for each delay (time instance from the last key frame), but right now we are just averaging over time and using a single weight for each rate. Although the results suggest the first approach works better, it seems that the weighting needs to be normalised as the gradients become unstable if the weights are too large. In the first method, the gradients became unstable after \\( \\mu_r \u003e 8 \\) but could be much lower in theory as the second method became unstable after \\( \\mu_r \u003e 4 \\). This is not surprising as the weighting scales the gradient.\nAblation study In terms of how codewords are generated, there are two methods:\nWe can simply take the latent vector of each frame \\( c_t = z_t \\), and use the predictor \\( Q \\) to determine how many tokens to transmit. We can obtain the codeword conditioned on the previous frames \\( c_t = f(z_t | z_{t-1},…,z_0) \\). The results of this study showed that the conditional codewords perform better, likely because the encoder can learn to discard unnecessary information if it knows what has already been transmitted.\nIn terms of the predictor:\nTake the conditional codewords and use a MLP to predict the quality of each rate. Use the input of the conditional codeword transformer and use a separate transformer to predict the quality at each rate. Use the output of the conditional codeword transformer and run a second transformer on those codewords conditioned on the prior codewords. Both the second and third methods work well. To reduce the memory usage during training, the second method is preferred.\nIn terms of references for decoding, the following two approaches are considered:\nUsing the decoded frames to train a separate auto-feature encoder Use the noisy codewords from the prior transmission directly Likewise, for encoding, the encoder can either use zeros to initialize the prior frame latent conditioning, or use learned features obtained from mapping all zero frames to the latent. The second approach uses less memory and is equally good.\nBandwidth allocation Having obtained a predictor/entropy model of the tokens, we would now like to allocate bandwidth according to this information. Chenghong has a method where we input the bandwidth ratio into a MLP along with the tokens and that allows the MLP to learn to allocate bandwidth accordingly. However, we would like the bandwidth input to also be a learn-able parameter. The point is to avoid ordered information, as currently the predictor/entropy model tells us how many tokens are needed to achieve a certain quality if the tokens were transmitted sequentially. This has proven to be a performance degrader, compared to unordered encoding.\n","wordCount":"1615","inLanguage":"en","datePublished":"2023-01-04T00:00:00Z","dateModified":"2023-01-04T10:29:08Z","author":[{"@type":"Person","name":"Tze-Yang Tung"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/"},"publisher":{"@type":"Organization","name":"My Notebook","logo":{"@type":"ImageObject","url":"https://r0ymanesco.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://r0ymanesco.github.io/my-notes accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://r0ymanesco.github.io/my-notes/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://r0ymanesco.github.io/my-notes>Home</a>&nbsp;»&nbsp;<a href=https://r0ymanesco.github.io/my-notes/projects/>Projects</a></div><h1 class=post-title>DeepJSCC Stream</h1><div class=post-meta><span title='2023-01-04 00:00:00 +0000 UTC'>4 January, 2023</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1615 words&nbsp;·&nbsp;Tze-Yang Tung</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#deepjscc_stream_a>Using transformers to autoregressively predict the next codeword</a><ul><li><a href=#deepjscc_stream_a1>Poor results</a></li><li><a href=#deepjscc_stream_a2>New encode/decode scheme</a></li><li><a href=#deepjscc_stream_a3>Connection to R-D</a></li></ul></li><li><a href=#deepjscc_stream_b>Snapshot compressive imaging (SCI)</a></li><li><a href=#deepjscc_stream_c>VCT based R-D prediction</a><ul><li><a href=#deepjscc_stream_c1>Compression quality prediction</a></li><li><a href=#deepjscc_stream_c2>Architecture</a></li><li><a href=#deepjscc_stream_c3>Hyperparameters</a></li><li><a href=#deepjscc_stream_c4>Ablation study</a></li><li><a href=#deepjscc_stream_c5>Bandwidth allocation</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=deepjscc_stream_a>Using transformers to autoregressively predict the next codeword<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_a>#</a></h2><h3 id=deepjscc_stream_a1>Poor results<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_a1>#</a></h3><p>The encoding/decoding approach used so far has not worked:</p><ul><li>The implementation that uses 2D transformers eventually diverged and crashed.</li><li>The impelmentation using <a href=https://arxiv.org/abs/2206.07307 target=_blank>VTC transformers</a>
stopped improving and produced bad results, weaker than the 2D transformers model.</li></ul><p>However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work.
It is also much faster to train.
It would make more sense to try to improve the current implementation.</p><h3 id=deepjscc_stream_a2>New encode/decode scheme<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_a2>#</a></h3><p>What transformers are really good at is autoregression.
So the question we want to pose to a transformer is: &ldquo;given the prior tokens, what is the next token?&rdquo;</p><p>Given frames \( \{x_1, &mldr;, x_n\} \), with corresponding features \( y_i = E(x_i) \).</p><ul><li>Encoder: learn a &ldquo;query&rdquo; token \( Q_z \) that learns the codeword given a sequence of frame features \( \{y_1,&mldr;,y_n\} \).
That is, we are modelling \( P(z_n | y_1, &mldr;, y_n) \) by querying the self-attention output of \( \{y_1,&mldr;,y_n\} \).
This can be done sequentially as frames arrive and we can observe the entropy of \( z_n \) to determine the GoP cut-off.
<strong>Causality is naturally preserved</strong>.
<strong>Note that this is NOT an autoregressive process</strong>.
If we want the codeword to be autoregressive, we can condition on \( (y_i, z_{n-1}) \).</li><li>Decoder: given \( z_n \), learn a &ldquo;start decode&rdquo; token \( \hat{y}_0 \), such that we autoregressively decode the frame features.
That is, we are modelling \( P(\hat{y}_n | \hat{y}_0,&mldr;, \hat{y}_{n-1}; z_n) \).
The estimated frame can then be obtained as \( \hat{x}_n = D(\hat{y}_n) \).
<strong>To ensure causality, the decoding should be masked</strong>.</li></ul><p>Unfortunately the new design still does not work.
The next step is to try an autoregressive encoder.
The encoding will be slower due to the iterative nature of the code but it may be better due to the progressiveness rather than single step coding.</p><p>Another option is to implement the same idea but using 2D transformers.
This is easier so will go first.</p><h3 id=deepjscc_stream_a3>Connection to R-D<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_a3>#</a></h3><p>Given a channel bandwidth constraint, you could in theory traverse the R-D trade-off by considering,
given a target distortion, how many frames can I encode into the available channel resources?
The distortion would be lower-bounded by the distortion offered by encoding 1 frame.
More frames encoded implies lower rate (per frame) and therefore higher distortion.
This is in contrast to typical R-D trade-off, as this implies that greater latency (encoding more frames) increases distortion.</p><h2 id=deepjscc_stream_b>Snapshot compressive imaging (SCI)<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_b>#</a></h2><p>The <a href=http://dx.doi.org/10.1109/TPAMI.2022.3225382 target=_blank>STformer</a>
seems to be a good way to perform 2D transformer.
Moreover, the <a href=https://arxiv.org/pdf/2103.04421.pdf target=_blank>snapshot compressive imaging (SCI)</a>
problem, sounds very interesting and may be applicable to our problem.
In the SCI formulation, the encoding is done via the summation of frames, each multiplied with a mask.
Then given the mask, and the result of the summation, the decoder tries to invert the process.
This is akin to an under-determined inverse problem.
If we use this for our coding problem, this would essentially entail a simple encoder, where each frame is mapped to a feature and those features are summed together to form the codeword.
Then the decoder, knowing the filters used in the encoding process, inverts the summation and reproduces the original frames.</p><p>SCI is basically an inverse problem (very similar to compressive sensing).
We can use a transformer to learn the masks for each feature tensor and sum them up.
The receiver then computes the inverse problem given the &ldquo;measurements&rdquo; (masks).
If we want to achieve autoregressive coding, we can just <strong>condition the next mask on the previous masks</strong>.
The decoder can be like the <a href=http://dx.doi.org/10.1109/TPAMI.2022.3225382 target=_blank>STformer</a></p><p>This idea has been shelved for now as it requires the masks to be sent separately.</p><h2 id=deepjscc_stream_c>VCT based R-D prediction<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_c>#</a></h2><p>We can use the same model as VCT to predict the distribution and use it to determine the number of channel uses.
Then a second transformer would predict the image/video quality instead of the distribution.
Once both transformers are trained, we can traverse the R-D curve freely.
That is, we first map frames into the latent space via an encoder \( z_t = E(x_t) \).
Then, we want to estimate \( Q(z_t) = f_\theta(z_{t-1},&mldr;,z_0) \), where \( Q \) is the quality function, parameterised by \( \theta \).</p><p>To generate codewords, we condition the current frame&rsquo;s latent vector on the previous frames&rsquo; latent vectors \( c_t = f(z_t | z_{t-1},&mldr;,z_0) \), using a transformer.</p><p>Currently, the coding stage uses reconstructions from the receiver at a random rate to infer the current frame.
This might lead to suboptimal results as initial reconstructions are poor, therefore affecting the reconstructions of future frames.
However, accumulating gradients across the entire video sequence would require too much memory.
Perhaps there is a way to do curriculum training, starting with feature encoder only before jointly training with the transformer, to find a middle ground.</p><p>In order to preserve the ordering of rate-distortion curve, the output of the predictor is a cumulative sum.
The predictor training using a transformer conditioned on prior frames does indeed work.
The question is whether the accuracy is good enough to determine the rate.
This result is also based on codewords which are the output of the feature encoder, without prior frame conditioning.</p><h3 id=deepjscc_stream_c1>Compression quality prediction<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_c1>#</a></h3><p>The same concept can be used to predict the compression quality for a given number of bits.
The VCT paper only considers average quality for a given number of bits.
If we allow variable rate compression, then we can instead target a fixed quality and adjust the bit-rate to obtain it.
The result would be <strong>for a chosen distortion, an average bit-rate to achieve it</strong>.</p><h3 id=deepjscc_stream_c2>Architecture<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_c2>#</a></h3><p>According to the <a href=https://arxiv.org/abs/2206.07307 target=_blank>VCT video compression</a>
paper, it is possible to train jointly with this architecture using the architecture in <a href=http://dx.doi.org/10.1109/CVPR52688.2022.00563 target=_blank>ELIC</a>
.
Preliminary results seems to suggest a pretty substantial improvement using this architecture, over the one proposed by VCT.
The primary difference is that the GDN layers are replaced by residual layers.
It is interesting to see that the GDN layer is becoming out of fashion.</p><p>On the other hand, the latent residual predictor proposed by the VCT paper was not useful for our case since the codeword and the decoder both already condition on the previous frames as well as the current frame tokens.</p><h3 id=deepjscc_stream_c3>Hyperparameters<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_c3>#</a></h3><p>In the <a href=http://dx.doi.org/10.1109/CVPR52688.2022.00563 target=_blank>ELIC</a>
paper, they suggest using dynamic weights on the loss of different rates to improve the generalisation for each rate.
There are two approaches:</p><ul><li>The idea is that, let \( D_r \) be the average distortion of the model at rate \( r \), then if the average improves by more than a constant factor \( \delta \), then the weighting \( \mu_r \) at that rate will decrease, and vice versa.
The loss is then \( \sum_r \mu_r D_r \).</li><li>Another approach is to compare frames at successive rates and ensure that the improvement gap between each successive rate is sufficiently large.</li></ul><p>The paper actually suggests using a different rate for each delay (time instance from the last key frame), but right now we are just averaging over time and using a single weight for each rate.
Although the results suggest the first approach works better, it seems that the weighting needs to be normalised as the gradients become unstable if the weights are too large.
In the first method, the gradients became unstable after \( \mu_r > 8 \) but could be much lower in theory as the second method became unstable after \( \mu_r > 4 \).
This is not surprising as the weighting scales the gradient.</p><h3 id=deepjscc_stream_c4>Ablation study<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_c4>#</a></h3><p>In terms of how codewords are generated, there are two methods:</p><ul><li>We can simply take the latent vector of each frame \( c_t = z_t \), and use the predictor \( Q \) to determine how many tokens to transmit.</li><li>We can obtain the codeword conditioned on the previous frames \( c_t = f(z_t | z_{t-1},&mldr;,z_0) \).</li></ul><p>The results of this study showed that the conditional codewords perform better, likely because the encoder can learn to discard unnecessary information if it knows what has already been transmitted.</p><p>In terms of the predictor:</p><ul><li>Take the conditional codewords and use a MLP to predict the quality of each rate.</li><li>Use the input of the conditional codeword transformer and use a separate transformer to predict the quality at each rate.</li><li>Use the output of the conditional codeword transformer and run a second transformer on those codewords <strong>conditioned</strong> on the prior codewords.</li></ul><p>Both the second and third methods work well. To reduce the memory usage during training, the second method is preferred.</p><p>In terms of references for decoding, the following two approaches are considered:</p><ul><li>Using the decoded frames to train a separate auto-feature encoder</li><li>Use the noisy codewords from the prior transmission directly</li></ul><p>Likewise, for encoding, the encoder can either use zeros to initialize the prior frame latent conditioning, or use learned features obtained from mapping all zero frames to the latent.
The second approach uses less memory and is equally good.</p><h3 id=deepjscc_stream_c5>Bandwidth allocation<a hidden class=anchor aria-hidden=true href=#deepjscc_stream_c5>#</a></h3><p>Having obtained a predictor/entropy model of the tokens, we would now like to allocate bandwidth according to this information.
Chenghong has a method where we input the bandwidth ratio into a MLP along with the tokens and that allows the MLP to learn to allocate bandwidth accordingly.
However, we would like the bandwidth input to also be a learn-able parameter.
The point is to avoid ordered information, as currently the predictor/entropy model tells us how many tokens are needed to achieve a certain quality if the tokens were transmitted sequentially.
This has proven to be a performance degrader, compared to unordered encoding.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://r0ymanesco.github.io/my-notes/tags/projects/>Projects</a></li></ul><nav class=paginav><a class=prev href=https://r0ymanesco.github.io/my-notes/projects/server_migration/><span class=title>« Prev</span><br><span>Server Migration</span></a>
<a class=next href=https://r0ymanesco.github.io/my-notes/projects/goal_oriented_coding/><span class=title>Next »</span><br><span>Goal-oriented Coding</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://r0ymanesco.github.io/my-notes>My Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>