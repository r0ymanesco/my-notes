<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Server Migration | My Notebook</title><meta name=keywords content="Projects"><meta name=description content="Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.
To solve these problems, the new approach will split the problem into 3 sub-problems:
User management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once."><meta name=author content="Tze-Yang Tung"><link rel=canonical href=https://r0ymanesco.github.io/my-notes/projects/server_migration/><link crossorigin=anonymous href=/my-notes/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-notes/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://r0ymanesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://r0ymanesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://r0ymanesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://r0ymanesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://r0ymanesco.github.io/my-notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><meta property="og:title" content="Server Migration"><meta property="og:description" content="Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.
To solve these problems, the new approach will split the problem into 3 sub-problems:
User management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once."><meta property="og:type" content="article"><meta property="og:url" content="https://r0ymanesco.github.io/my-notes/projects/server_migration/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-01-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-05T15:09:42+00:00"><meta property="og:site_name" content="My Notebook"><meta name=twitter:card content="summary"><meta name=twitter:title content="Server Migration"><meta name=twitter:description content="Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.
To solve these problems, the new approach will split the problem into 3 sub-problems:
User management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Projects","item":"https://r0ymanesco.github.io/my-notes/projects/"},{"@type":"ListItem","position":3,"name":"Server Migration","item":"https://r0ymanesco.github.io/my-notes/projects/server_migration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Server Migration","name":"Server Migration","description":"Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.\nTo solve these problems, the new approach will split the problem into 3 sub-problems:\nUser management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once.","keywords":["Projects"],"articleBody":"Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.\nTo solve these problems, the new approach will split the problem into 3 sub-problems:\nUser management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once. We also want to reduce user dependency on the libraries installed on the servers themselves in order to make software updates easier. Finally, in order to support the skeleton approach to server libraries, we will use Docker to launch jobs on a specific machine of the user’s choice. The dashboard currently being used to monitor GPU utilisation can continue to exist so that users know which machines are free to launch jobs to. The next section outlines how these objectives will be accompolished.\nSolution Firstly, to facilitate this transition, users should be notified of a timeline for doing so and server managers should present a timetable for how this transition will be carried out. I suggest migrating servers in batches to this new architecture to reduce disruption. Users should be asked to backup their data.\nUser management In order to centralise user management, we will use the LDAP system available on all Linux distributions. The system uses a host server to store user login information where client servers can authenticate against. This way, new users only need to be added to the host server. Any machine can be the host but I recommend using denizpower as it already hosts the dashboard.\nFor storage, we will take all of the hard drives that are in each individual server and integrate them into a network drive (e.g., NAS or just attach them to the LDAP host server). Every user would store their data in this large hard drive so that we don’t run out of space on the system drive in the client servers.\nLibrary management Previously, due to the wide variety of Linux distributions in the cluster, each machine had different versions of the kernel, NVIDIA drivers, …etc, which is extremely painful to update. In fact, for all intents and purposes, we did not update machines at all after the initial installation.\nTo make this process easier, we will instead only install the minimum amount of libraries on each server and rely on Docker to deploy jobs with specific library requirements (see below ). The minimum software requirement on each server are the following:\nBase Linux distribution (I recommend using Archlinux because it has very comprehensive documentation for LDAP and Docker). Docker (make sure the service is enabled after installation) NVIDIA drivers (might not be needed, see below ) Job deployment As mentioned above, we will use Docker to deploy jobs. Docker is a containerised application that allows you to specify the code and required libraries to run the code, all completely independent from the base OS installation. This means that each user can simply build containers with the libraries that they need without having to install any of the libraries on the base OS itself. The lab can also create containers with some basic libraries, like specific Pytorch/Tensorflow versions, that other can use as the base and add additional libraries as required. These containers can be hosted on Docker Hub for everyone to download. You can also specify in your container to copy the necessary files from the central storage to the client server and remove it when the job is done so that the client drive doesn’t run out of space. It may even be possible to have the NVIDIA drivers installed within the Docker container itself (see here ), but it is not well documented and might require some experimentation. Users would check on the dashboard to see which server is available and deploy jobs to the selected server.\nDocker does require some effort to learn and get used to but in the long run, it will be worth it. It’s also a useful skill in industry as many tech companies use Docker to run jobs on their servers.\nNew servers As Deniz mentioned, we will be purchasing new servers soon. As someone that has gone through the purchasing process of many of the servers you see on the dashboard, I want to give my opinion on how I configure the hardware.\nFirstly, the person to contact about purchasing new servers is Tim Brown (email: Tim.Brown@pcspecialist.co.uk ). He is our contact at PC Specialist, which is Imperial’s contracted company for computer equipment purchases. You can go to PC Specialist’s website to configure a machine and send it to Tim for a quote. You can usually get a discount on the price.\nSecondly, try to put as many GPUs in a single machine as possible. This is because our jobs are not very CPU intensive so it doesn’t make sense to only put 1 or 2 GPUs in a machine. Each machine also takes up a considerable amount of space on the server rack so we want to be as space efficient as possible. However, when you configure machine on PC Specialist’s website, they may not let you put more than a certain number of GPUs in a machine. What you should do in that case is configure the maximum allowed by the website and then contact Tim to ask him what the maximum number allowed actually is. Often they will have other chassis available that can hold more GPUs.\nLastly, when striking a balance between GPU compute, memory and price, consider the use case first. So far, we have only purchased gaming grade GPUs, which offer at most 24GB memory and moderate performance. This is because they are very affordable for what they offer, and our computing needs have not been so extreme that these GPUs cannot handle them. Moreover, due to the low price, we can buy many of them so that we have a large number of GPUs for everyone to use.\nHowever, recently there has been increasing demand for compute as some people have begun to train reasonably large Transformer models. Therefore, it might make sense to purchase a few professional grade GPUs, like the NVIDIA RTX 6000 (48GB) . Bare in mind that such GPUs are extremely expensive and can itself cost as much as a single server that we have currently.\nAt the end of the day, it comes down to the budget and use cases. If you want to ask me questions, you can reach me at my email: tzeyang.tung.work@gmail.com ","wordCount":"1128","inLanguage":"en","datePublished":"2023-01-05T00:00:00Z","dateModified":"2023-01-05T15:09:42Z","author":[{"@type":"Person","name":"Tze-Yang Tung"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://r0ymanesco.github.io/my-notes/projects/server_migration/"},"publisher":{"@type":"Organization","name":"My Notebook","logo":{"@type":"ImageObject","url":"https://r0ymanesco.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://r0ymanesco.github.io/my-notes accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://r0ymanesco.github.io/my-notes/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://r0ymanesco.github.io/my-notes>Home</a>&nbsp;»&nbsp;<a href=https://r0ymanesco.github.io/my-notes/projects/>Projects</a></div><h1 class=post-title>Server Migration</h1><div class=post-meta><span title='2023-01-05 00:00:00 +0000 UTC'>5 January, 2023</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1128 words&nbsp;·&nbsp;Tze-Yang Tung</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#server_migration_a>Architecture</a></li><li><a href=#server_migration_b>Solution</a><ul><li><a href=#server_migration_b1>User management</a></li><li><a href=#server_migration_b2>Library management</a></li><li><a href=#server_migration_b3>Job deployment</a></li></ul></li><li><a href=#server_migration_c>New servers</a></li></ul></nav></div></details></div><div class=post-content><h2 id=server_migration_a>Architecture<a hidden class=anchor aria-hidden=true href=#server_migration_a>#</a></h2><p>The current architecture of independent servers with a dashboard for monitoring usage is unsustainable.
As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added.
Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.</p><p>To solve these problems, the new approach will split the problem into 3 sub-problems:</p><ul><li><a href=#server_migration_b1>User management</a></li><li><a href=#server_migration_b2>Library management</a></li><li><a href=#server_migration_b3>Job deployment</a></li></ul><p>The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once.
We also want to reduce user dependency on the libraries installed on the servers themselves in order to make software updates easier.
Finally, in order to support the skeleton approach to server libraries, we will use Docker to launch jobs on a specific machine of the user&rsquo;s choice.
The dashboard currently being used to monitor GPU utilisation can continue to exist so that users know which machines are free to launch jobs to.
The next section outlines how these objectives will be accompolished.</p><h2 id=server_migration_b>Solution<a hidden class=anchor aria-hidden=true href=#server_migration_b>#</a></h2><p>Firstly, to facilitate this transition, users should be notified of a timeline for doing so and server managers should present a timetable for how this transition will be carried out.
I suggest migrating servers in batches to this new architecture to reduce disruption.
Users should be asked to backup their data.</p><h3 id=server_migration_b1>User management<a hidden class=anchor aria-hidden=true href=#server_migration_b1>#</a></h3><p>In order to centralise user management, we will use the <a href=https://wiki.archlinux.org/title/LDAP_authentication#Client_Setup target=_blank>LDAP</a>
system available on all Linux distributions.
The system uses a host server to store user login information where client servers can authenticate against.
This way, new users only need to be added to the host server.
Any machine can be the host but I recommend using denizpower as it already hosts the dashboard.</p><p>For storage, we will take all of the hard drives that are in each individual server and integrate them into a network drive (e.g., <a href=https://en.wikipedia.org/wiki/Network-attached_storage target=_blank>NAS</a>
or just attach them to the LDAP host server).
Every user would store their data in this large hard drive so that we don&rsquo;t run out of space on the system drive in the client servers.</p><h3 id=server_migration_b2>Library management<a hidden class=anchor aria-hidden=true href=#server_migration_b2>#</a></h3><p>Previously, due to the wide variety of Linux distributions in the cluster, each machine had different versions of the kernel, NVIDIA drivers, &mldr;etc, which is extremely painful to update.
In fact, for all intents and purposes, we did not update machines at all after the initial installation.</p><p>To make this process easier, we will instead only install the minimum amount of libraries on each server and rely on Docker to deploy jobs with specific library requirements (see <a href=#server_migration_b3>below</a>
).
The minimum software requirement on each server are the following:</p><ul><li>Base Linux distribution (I recommend using <a href=https://archlinux.org/ target=_blank>Archlinux</a>
because it has very comprehensive documentation for LDAP and Docker).</li><li><a href=https://www.docker.com/ target=_blank>Docker</a>
(make sure the service is enabled after installation)</li><li>NVIDIA drivers (might not be needed, see <a href=#server_migration_b3>below</a>
)</li></ul><h3 id=server_migration_b3>Job deployment<a hidden class=anchor aria-hidden=true href=#server_migration_b3>#</a></h3><p>As mentioned above, we will use <a href=https://www.docker.com/ target=_blank>Docker</a>
to deploy jobs.
Docker is a containerised application that allows you to specify the code and required libraries to run the code, all completely independent from the base OS installation.
This means that each user can simply build containers with the libraries that they need without having to install any of the libraries on the base OS itself.
The lab can also create containers with some basic libraries, like specific Pytorch/Tensorflow versions, that other can use as the base and add additional libraries as required.
These containers can be hosted on <a href=https://index.docker.io/ target=_blank>Docker Hub</a>
for everyone to download.
You can also specify in your container to copy the necessary files from the central storage to the client server and remove it when the job is done so that the client drive doesn&rsquo;t run out of space.
It may even be possible to have the NVIDIA drivers installed within the Docker container itself (see <a href=https://github.com/NVIDIA/nvidia-docker/issues/871 target=_blank>here</a>
), but it is not well documented and might require some experimentation.
Users would check on the dashboard to see which server is available and deploy jobs to the selected server.</p><p>Docker does require some effort to learn and get used to but in the long run, it will be worth it.
It&rsquo;s also a useful skill in industry as many tech companies use Docker to run jobs on their servers.</p><h2 id=server_migration_c>New servers<a hidden class=anchor aria-hidden=true href=#server_migration_c>#</a></h2><p>As Deniz mentioned, we will be purchasing new servers soon.
As someone that has gone through the purchasing process of many of the servers you see on the dashboard, I want to give my opinion on how I configure the hardware.</p><p>Firstly, the person to contact about purchasing new servers is Tim Brown (email: <a href=mailto:Tim.Brown@pcspecialist.co.uk>Tim.Brown@pcspecialist.co.uk</a>
).
He is our contact at PC Specialist, which is Imperial&rsquo;s contracted company for computer equipment purchases.
You can go to PC Specialist&rsquo;s <a href=https://www.pcspecialist.co.uk/ target=_blank>website</a>
to configure a machine and send it to Tim for a quote.
You can usually get a discount on the price.</p><p>Secondly, try to put as many GPUs in a single machine as possible.
This is because our jobs are not very CPU intensive so it doesn&rsquo;t make sense to only put 1 or 2 GPUs in a machine.
Each machine also takes up a considerable amount of space on the server rack so we want to be as space efficient as possible.
However, when you configure machine on PC Specialist&rsquo;s website, they may not let you put more than a certain number of GPUs in a machine.
What you should do in that case is configure the maximum allowed by the website and then contact Tim to ask him what the maximum number allowed actually is.
Often they will have other chassis available that can hold more GPUs.</p><p>Lastly, when striking a balance between GPU compute, memory and price, consider the use case first.
So far, we have only purchased gaming grade GPUs, which offer at most 24GB memory and moderate performance.
This is because they are very affordable for what they offer, and our computing needs have not been so extreme that these GPUs cannot handle them.
Moreover, due to the low price, we can buy many of them so that we have a large number of GPUs for everyone to use.</p><p>However, recently there has been increasing demand for compute as some people have begun to train reasonably large Transformer models.
Therefore, it might make sense to purchase a few professional grade GPUs, like the <a href=https://www.scan.co.uk/products/48gb-pny-nvidia-rtx-a6000-pcie-40-x16-ampere-10752-core-336-tensor-84-rt-cores-gddr6-w-ecc-dp target=_blank>NVIDIA RTX 6000 (48GB)</a>
.
Bare in mind that such GPUs are extremely expensive and can itself cost as much as a single server that we have currently.</p><p>At the end of the day, it comes down to the budget and use cases.
If you want to ask me questions, you can reach me at my email: <a href=mailto:tzeyang.tung.work@gmail.com>tzeyang.tung.work@gmail.com</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://r0ymanesco.github.io/my-notes/tags/projects/>Projects</a></li></ul><nav class=paginav><a class=next href=https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/><span class=title>Next »</span><br><span>DeepJSCC Stream</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://r0ymanesco.github.io/my-notes>My Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>