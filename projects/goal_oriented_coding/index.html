<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Goal-oriented Coding | My Notebook</title><meta name=keywords content="Projects"><meta name=description content="Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.
Applications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample."><meta name=author content="Tze-Yang Tung"><link rel=canonical href=https://r0ymanesco.github.io/my-notes/projects/goal_oriented_coding/><link crossorigin=anonymous href=/my-notes/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-notes/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://r0ymanesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://r0ymanesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://r0ymanesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://r0ymanesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://r0ymanesco.github.io/my-notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><meta property="og:title" content="Goal-oriented Coding"><meta property="og:description" content="Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.
Applications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample."><meta property="og:type" content="article"><meta property="og:url" content="https://r0ymanesco.github.io/my-notes/projects/goal_oriented_coding/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-01-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-04T11:50:03+00:00"><meta property="og:site_name" content="My Notebook"><meta name=twitter:card content="summary"><meta name=twitter:title content="Goal-oriented Coding"><meta name=twitter:description content="Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.
Applications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Projects","item":"https://r0ymanesco.github.io/my-notes/projects/"},{"@type":"ListItem","position":3,"name":"Goal-oriented Coding","item":"https://r0ymanesco.github.io/my-notes/projects/goal_oriented_coding/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Goal-oriented Coding","name":"Goal-oriented Coding","description":"Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.\nApplications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample.","keywords":["Projects"],"articleBody":"Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.\nApplications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample. Although compression using diffusion models has been demonstrated already , their approach relies on a latent variable with which the decoder is conditioned on, in order to guide decoder. However, this is a tall ask for a single, one step, latent variable. Typically, diffusion models will condition on all of the latents in each step, whereas here they are only conditioning on a single variable generated in one step. This single variable is then quantised and coded as many prior compression works have used, e.g., uniform noise to approximate rounding error and a hyperprior density estimation.\nA better approach would be to transmit those latent vectors in each step using universal quantisation. Starting from the highest step, where the posterior is almost an isotropic Gaussian distribution, we send a sample from this distribution using universal quantisation and the decoder tries to approximate likelihood function. \\( \\forall t, x_{t-1} \\Rightarrow q(x_t | x_{t-1}) \\stackrel{x_t}{\\Rightarrow} p(x_{t-1} | x_t) \\Rightarrow \\hat{x}_{t-1} \\), compute VAE loss \\( l_{ELBO}(x_{t-1}, \\hat{x}_{t-1}) \\). Moreover, the coding rate should be approximately constant since at each step \\( q(x_t | x_{t-1}) \\approx p(x_{t-1} | x_t) \\), therefore the KL divergence should be small. Essentially, the cost of coding is amortised into the diffusion steps. Even though this doesn’t make the coding speed any faster, it does provide a way to make rate-distortion trade-offs since we can stop coding after a certain point and obtain the intermediate image.\nAnother application is to consider a stochastic process which is conditional on a common random variable and the encoder wishes to communicate a sample of the stochastic process to a decoder. This could be an interesting goal-oriented communication problem .\nAnother interesting possibility is that because both the sender and the receiver are required to have a common random source, could the seed be used as a secret key for symmetric encryption? Basically, without the common random source, is it possible to decode successfully, even if approximately? (might be hard to prove the security of the scheme although maybe some kind of privacy guarantees can be made)\nAnother possibility is posterior matching . Interestingly, there are some similarities between posterior matching and universal quantisation . In posterior matching, the next transmitted random variable \\( X_{n+1} \\) is independent of the previous received values \\( Y^n \\), to ensure that it represents new information. Moreover, the message \\( \\theta \\) can be almost surely uniquely recovered from \\( (X_{n+1}, Y^n) \\) as \\( n \\rightarrow \\infty \\). An important theorem that universal quantisation relies on is the strong functional representation (SFR) lemma, which states that for general \\( (X, Y) \\), there exists a \\( Z \\) that is independent of \\( X \\) and \\( Y \\), and is a function of \\( (X, Z) \\), such that \\( H(Y | Z) \\leq I(X; Y) + \\log(I(X; Y) + 1) + 4 \\). Therefore, if we map \\( Z \\) to \\( X_{n+1} \\) in the previous problem and \\( Y \\mapsto \\theta \\), \\( X \\mapsto Y^n \\), then we have \\( H(\\theta | X_{n+1}) \\leq I(Y^n; \\theta) + \\log(I(Y^n; \\theta) + 1) + 4 \\). We can interpret \\( H(\\theta | X_{n+1}) \\) as any further uncertainty about the message after transmitting \\( X_{n+1} \\) and \\( I(Y^n; \\theta) \\) as knowledge the receiver has gained already.\nWe can apply universal quantisation to the transmission of a stochastic policy to a remote agent for MDPs or POMDPs . That is, consider a controller which has policy \\( \\pi(a | s) \\), and wishes to transmit a random action \\( a \\sim \\pi(a | s) \\) to a remote agent to carry out. It is trivial to see that this corresponds exactly to the SFR lemma, where \\( Y \\mapsto a \\) and \\( X \\mapsto s \\). It corresponds to the “sending actions” policy we have considered in the past. A few scenarios to consider:\nIf we have an exact channel model \\( P(\\hat{a} | a) \\), is there a JSCC equivalent of this? What if we have noiseless feedback of \\( \\hat{a} \\)? What if the controller also needs to learn the optimal policy? In this case, it is possible that initially, we would communicate very little so that the remote agent simply takes random actions based on a prior distribution. As the controller learns based on the reward feedback of the agent’s actions, it then begins to communicate what it thinks is optimal. This can be done using the common randomness (prior). The agent can construct an empirical distribution of the policy over time and once it has a sufficiently good estimate, no more communication is required. In this paper , a similar problem considering contexual multi-armed bandits (CMAB) was solved. If we have a MA-POMDP, the problem can be seen as matching the joint distribution of the agents’ actions with the optimal policy over all the agents. Common randomness could also be useful in this case to reduce the communication load and could perhaps even be done with simply a correlated random variable . What if the action space is continuous? Universal quantisation could be used to quantise this to a finite rate? Can this be applied to a goal-oriented quantisation problem? This might be used in a sensor network. MA-POMDP Problem Consider a 2-agent POMDP , where the environment is governed by the transition kernel \\( P(s_1^t, s_2^t | a_1^{t-1}, a_2^{t-1}) \\) and the agents have policies \\( \\pi_i^t(a_i^t | s_i^t, m_i^t)\\; i \\in \\{0, 1\\} \\). Let’s assume that the union of the observations is in the state \\( s_1 \\cup s_2 \\in \\mathcal{S} \\). The messages \\( m_i^t \\) are generated by the other agent and sent to the opposite agent. We want to determine the minimum rate of the messages such that the agents converge to the optimal policy \\( \\pi^\\ast(a_1^t, a_2^t | s_1^t, s_2^t) \\).\nInitial thoughts It is valid to first consider what are the missing information for each agent to obtain stationarity. For each agent, they need to be able to know the policy and observation of the other agent in order to have the complete information necessary to observe a stationary POMDP. From the perspective of agent 1, if we know the transition kernel \\( P(s_1^t, s_2^t | a_1^{t-1}, a_2^{t-1}) \\), then if we know the action taken by agent 2, we can code the observation of agent 2 using \\( P(s_2^t | a_1^{t-1}, a_2^{t-1}) \\) by marginalising over \\( s_1^t \\). In order to send the action, if we are given common randomness, then we can initialise both policies using the same random variable so that at \\( t = 1 \\), each agent simply codes the action taken based on \\( \\pi_i^1(a_i^1) \\). For \\( t \u003e 1 \\), instead of coding the action based on \\( \\pi_i^t(a_i^t | s_i^t) \\), we can use relative entropy coding to send a sample \\( a_i^t \\sim \\pi_i^t(a_i^t | s_i^t) \\) using \\( 2^\\epsilon \\) samples and \\( \\epsilon \\) bits, where \\( \\epsilon = D_{KL}(\\pi_i^t || \\pi_i^{t-1}) \\). Moreover, if we use a update policy like trust region policy optimisation (TRPO), then it is guaranteed that \\( D_{KL}(\\pi_i^t || \\pi_i^{t-1}) \\leq \\epsilon \\). Knowing the action and the observation of the other agent, the agent can update its belief of the other agent’s policy exactly the same as the other agent. Since the belief matches the policy exactly, \\( \\pi_i^t \\) becomes the new common randomness. Therefore, in each round, the agents only need to send the amount of update that their policies changed by. In fact, if we use something similar to epsilon greedy for exploration, then the prior chosen at the beginning would represent the exploration distribution and while epsilon is large at the beginning, fewer communication would be needed, and as epsilon gets smaller we would communicate more.\nThe thing that needs to be checked is whether or not no common randomness can achieve the same thing. If no common prior on the policy is given at \\( t = 1 \\), then each agent can only form a belief that may not be the same as the policy of the other agent (private randomness). Therefore, to send the action, the agent must use at least \\( H(a_i^t | s_i^t) \\) bits, based on the current policy \\( \\pi_i^t \\). This can be significantly bigger than \\( \\epsilon \\). The difference would be even greater if the action space was continuous since the private randomness version would need to quantize, leading to loss in performance.\nThis project also merges well with deep RL since the agent policies can be parameterized by DNNs.\n","wordCount":"1523","inLanguage":"en","datePublished":"2023-01-04T00:00:00Z","dateModified":"2023-01-04T11:50:03Z","author":[{"@type":"Person","name":"Tze-Yang Tung"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://r0ymanesco.github.io/my-notes/projects/goal_oriented_coding/"},"publisher":{"@type":"Organization","name":"My Notebook","logo":{"@type":"ImageObject","url":"https://r0ymanesco.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://r0ymanesco.github.io/my-notes accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://r0ymanesco.github.io/my-notes/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://r0ymanesco.github.io/my-notes>Home</a>&nbsp;»&nbsp;<a href=https://r0ymanesco.github.io/my-notes/projects/>Projects</a></div><h1 class=post-title>Goal-oriented Coding</h1><div class=post-meta><span title='2023-01-04 00:00:00 +0000 UTC'>4 January, 2023</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1523 words&nbsp;·&nbsp;Tze-Yang Tung</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#universal_coding_a>Project goal</a></li><li><a href=#universal_coding_b>Applications</a></li><li><a href=#universal_coding_c>MA-POMDP Problem</a><ul><li><a href=#universal_coding_c1>Initial thoughts</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=universal_coding_a>Project goal<a hidden class=anchor aria-hidden=true href=#universal_coding_a>#</a></h2><p>This project aims to leverage <a href=/my-notes/posts/information_theory/#information_theory_d2>universal quantisation</a>
for goal-oriented coding.
It is closely related to the <a href=/my-notes/posts/information_theory/#information_theory_b>rate-distortion-perception</a>
, as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.</p><h2 id=universal_coding_b>Applications<a hidden class=anchor aria-hidden=true href=#universal_coding_b>#</a></h2><p>A potential application for <a href=/my-notes/posts/information_theory/#information_theory_d2>universal quantisation</a>
in compression is in diffusion models.
Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample.
Although compression using diffusion models has been <a href=https://arxiv.org/abs/2209.06950 target=_blank>demonstrated already</a>
, their approach relies on a latent variable with which the decoder is conditioned on, in order to guide decoder.
However, this is a tall ask for a single, one step, latent variable.
Typically, diffusion models will condition on all of the latents in each step, whereas here they are only conditioning on a single variable generated in one step.
This single variable is then quantised and coded as many prior compression works have used, e.g., uniform noise to approximate rounding error and a hyperprior density estimation.</p><p>A better approach would be to transmit those latent vectors in each step using universal quantisation.
Starting from the highest step, where the posterior is almost an isotropic Gaussian distribution, we send a sample from this distribution using universal quantisation and the decoder tries to approximate likelihood function.
\( \forall t, x_{t-1} \Rightarrow q(x_t | x_{t-1}) \stackrel{x_t}{\Rightarrow} p(x_{t-1} | x_t) \Rightarrow \hat{x}_{t-1} \), compute VAE loss \( l_{ELBO}(x_{t-1}, \hat{x}_{t-1}) \).
Moreover, the coding rate should be approximately constant since at each step \( q(x_t | x_{t-1}) \approx p(x_{t-1} | x_t) \), therefore the KL divergence should be small.
Essentially, the cost of coding is amortised into the diffusion steps.
Even though this doesn&rsquo;t make the coding speed any faster, it does provide a way to make rate-distortion trade-offs since we can stop coding after a certain point and obtain the intermediate image.</p><p>Another application is to consider a stochastic process which is conditional on a common random variable and the encoder wishes to communicate a sample of the stochastic process to a decoder.
This could be an interesting <a href=/my-notes/posts/semantic_communications/#semantic_communications_c>goal-oriented communication problem</a>
.</p><p>Another interesting possibility is that because both the sender and the receiver are required to have a common random source, could the seed be used as a secret key for symmetric encryption?
Basically, without the common random source, is it possible to decode successfully, even if approximately?
(might be hard to prove the security of the scheme although maybe some kind of privacy guarantees can be made)</p><p>Another possibility is <a href=/my-notes/posts/information_theory/#information_theory_e1>&lt;strong>posterior matching&lt;/strong></a>
.
Interestingly, there are some similarities between posterior matching and <a href=/my-notes/posts/information_theory/#information_theory_d2>universal quantisation</a>
.
In posterior matching, the next transmitted random variable \( X_{n+1} \) is independent of the previous received values \( Y^n \), to ensure that it represents new information.
Moreover, the message \( \theta \) can be almost surely uniquely recovered from \( (X_{n+1}, Y^n) \) as \( n \rightarrow \infty \).
An important theorem that <a href=/my-notes/posts/information_theory/#information_theory_d2>universal quantisation</a>
relies on is the strong functional representation <a href=http://dx.doi.org/10.1109/TIT.2018.2865570 target=_blank>(SFR)</a>
lemma, which states that for general \( (X, Y) \), there exists a \( Z \) that is independent of \( X \) and \( Y \), and is a function of \( (X, Z) \), such that \( H(Y | Z) \leq I(X; Y) + \log(I(X; Y) + 1) + 4 \).
Therefore, if we map \( Z \) to \( X_{n+1} \) in the previous problem and \( Y \mapsto \theta \), \( X \mapsto Y^n \), then we have \( H(\theta | X_{n+1}) \leq I(Y^n; \theta) + \log(I(Y^n; \theta) + 1) + 4 \).
We can interpret \( H(\theta | X_{n+1}) \) as any further uncertainty about the message after transmitting \( X_{n+1} \) and \( I(Y^n; \theta) \) as knowledge the receiver has gained already.</p><p>We can apply <a href=/my-notes/posts/information_theory/#information_theory_d2>universal quantisation</a>
to the transmission of a stochastic policy to a remote agent for <a href=/my-notes/posts/reinforcement_learning/#reinforcement_learning_a>MDPs</a>
or <a href=/my-notes/posts/reinforcement_learning/#reinforcement_learning_b>POMDPs</a>
.
That is, consider a controller which has policy \( \pi(a | s) \), and wishes to transmit a random action \( a \sim \pi(a | s) \) to a remote agent to carry out.
It is trivial to see that this corresponds exactly to the SFR lemma, where \( Y \mapsto a \) and \( X \mapsto s \).
It corresponds to the &ldquo;sending actions&rdquo; policy we have considered in the past.
A few scenarios to consider:</p><ul><li>If we have an exact channel model \( P(\hat{a} | a) \), is there a JSCC equivalent of this?</li><li>What if we have noiseless feedback of \( \hat{a} \)?</li><li>What if the controller also needs to learn the optimal policy?
In this case, it is possible that initially, we would communicate very little so that the remote agent simply takes random actions based on a prior distribution.
As the controller learns based on the reward feedback of the agent&rsquo;s actions, it then begins to communicate what it thinks is optimal. This can be done using the common randomness (prior).
The agent can construct an empirical distribution of the policy over time and once it has a sufficiently good estimate, no more communication is required.
In this <a href=http://dx.doi.org/10.1109/ISIT50566.2022.9834399 target=_blank>paper</a>
, a similar problem considering contexual multi-armed bandits (CMAB) was solved.</li><li>If we have a MA-POMDP, the problem can be seen as matching the joint distribution of the agents&rsquo; actions with the optimal policy over all the agents.
<a href=/my-notes/posts/information_theory/#information_theory_b1>Common randomness</a>
could also be useful in this case to reduce the communication load and could perhaps even be done with simply a <a href=http://dx.doi.org/10.1109/GLOBECOM42002.2020.9322460 target=_blank>correlated random variable</a>
.</li><li>What if the action space is <strong>continuous</strong>? Universal quantisation could be used to quantise this to a finite rate? Can this be applied to a <strong>goal-oriented quantisation</strong> problem?
This might be used in a sensor network.</li></ul><h2 id=universal_coding_c>MA-POMDP Problem<a hidden class=anchor aria-hidden=true href=#universal_coding_c>#</a></h2><p>Consider a 2-agent <a href=/my-notes/posts/reinforcement_learning/#reinforcement_learning_b>POMDP</a>
, where the environment is governed by the transition kernel \( P(s_1^t, s_2^t | a_1^{t-1}, a_2^{t-1}) \) and the agents have policies \( \pi_i^t(a_i^t | s_i^t, m_i^t)\; i \in \{0, 1\} \).
Let&rsquo;s assume that the union of the observations is in the state \( s_1 \cup s_2 \in \mathcal{S} \).
The messages \( m_i^t \) are generated by the other agent and sent to the opposite agent.
We want to determine the minimum rate of the messages such that the agents converge to the optimal policy \( \pi^\ast(a_1^t, a_2^t | s_1^t, s_2^t) \).</p><h3 id=universal_coding_c1>Initial thoughts<a hidden class=anchor aria-hidden=true href=#universal_coding_c1>#</a></h3><p>It is valid to first consider what are the missing information for each agent to obtain stationarity.
For each agent, they need to be able to know the policy and observation of the other agent in order to have the complete information necessary to observe a stationary POMDP.
From the perspective of agent 1, if we know the transition kernel \( P(s_1^t, s_2^t | a_1^{t-1}, a_2^{t-1}) \), then if we know the action taken by agent 2, we can code the observation of agent 2 using \( P(s_2^t | a_1^{t-1}, a_2^{t-1}) \) by marginalising over \( s_1^t \).
In order to send the action, if we are given common randomness, then we can initialise both policies using the same random variable so that at \( t = 1 \), each agent simply codes the action taken based on \( \pi_i^1(a_i^1) \).
For \( t > 1 \), instead of coding the action based on \( \pi_i^t(a_i^t | s_i^t) \), we can use relative entropy coding to send a sample \( a_i^t \sim \pi_i^t(a_i^t | s_i^t) \) using \( 2^\epsilon \) samples and \( \epsilon \) bits, where \( \epsilon = D_{KL}(\pi_i^t || \pi_i^{t-1}) \).
Moreover, if we use a update policy like trust region policy optimisation (TRPO), then it is guaranteed that \( D_{KL}(\pi_i^t || \pi_i^{t-1}) \leq \epsilon \).
Knowing the action and the observation of the other agent, the agent can update its belief of the other agent&rsquo;s policy exactly the same as the other agent.
Since the belief matches the policy exactly, \( \pi_i^t \) becomes the new common randomness.
Therefore, in each round, the agents only need to send the amount of update that their policies changed by.
In fact, if we use something similar to epsilon greedy for exploration, then the prior chosen at the beginning would represent the exploration distribution and while epsilon is large at the beginning, fewer communication would be needed, and as epsilon gets smaller we would communicate more.</p><p>The thing that needs to be checked is whether or not no common randomness can achieve the same thing.
If no common prior on the policy is given at \( t = 1 \), then each agent can only form a belief that may not be the same as the policy of the other agent (private randomness).
Therefore, to send the action, the agent must use at least \( H(a_i^t | s_i^t) \) bits, based on the current policy \( \pi_i^t \).
This can be significantly bigger than \( \epsilon \).
The difference would be even greater if the action space was continuous since the private randomness version would need to quantize, leading to loss in performance.</p><p>This project also merges well with deep RL since the agent policies can be parameterized by DNNs.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://r0ymanesco.github.io/my-notes/tags/projects/>Projects</a></li></ul><nav class=paginav><a class=prev href=https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/><span class=title>« Prev</span><br><span>DeepJSCC Stream</span></a>
<a class=next href=https://r0ymanesco.github.io/my-notes/projects/graph_coding/><span class=title>Next »</span><br><span>Graph Coding</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://r0ymanesco.github.io/my-notes>My Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>