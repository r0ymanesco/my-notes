<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Information Theory | My Notebook</title><meta name=keywords content="ReadingList"><meta name=description content="Rate-distortion This is fairly straight forward, will complete later.
Universal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \(p\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.
d-semifaithful coding \(d\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \(d\) of the original source sequence with probability 1."><meta name=author content="Tze-Yang Tung"><link rel=canonical href=https://r0ymanesco.github.io/my-notes/posts/information_theory/><link crossorigin=anonymous href=/my-notes/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-notes/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://r0ymanesco.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://r0ymanesco.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://r0ymanesco.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://r0ymanesco.github.io/apple-touch-icon.png><link rel=mask-icon href=https://r0ymanesco.github.io/my-notes/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><meta property="og:title" content="Information Theory"><meta property="og:description" content="Rate-distortion This is fairly straight forward, will complete later.
Universal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \(p\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.
d-semifaithful coding \(d\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \(d\) of the original source sequence with probability 1."><meta property="og:type" content="article"><meta property="og:url" content="https://r0ymanesco.github.io/my-notes/posts/information_theory/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-04T12:02:09+00:00"><meta property="og:site_name" content="My Notebook"><meta name=twitter:card content="summary"><meta name=twitter:title content="Information Theory"><meta name=twitter:description content="Rate-distortion This is fairly straight forward, will complete later.
Universal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \(p\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.
d-semifaithful coding \(d\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \(d\) of the original source sequence with probability 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Topics","item":"https://r0ymanesco.github.io/my-notes/posts/"},{"@type":"ListItem","position":3,"name":"Information Theory","item":"https://r0ymanesco.github.io/my-notes/posts/information_theory/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Information Theory","name":"Information Theory","description":"Rate-distortion This is fairly straight forward, will complete later.\nUniversal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \\(p\\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.\nd-semifaithful coding \\(d\\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \\(d\\) of the original source sequence with probability 1.","keywords":["ReadingList"],"articleBody":"Rate-distortion This is fairly straight forward, will complete later.\nUniversal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \\(p\\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.\nd-semifaithful coding \\(d\\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \\(d\\) of the original source sequence with probability 1. A novel variation of universal \\(d\\)-semifaithful coding is in which the distortion measure is revealed to the encoder alone and only when it receives the source sequence \\(x^n\\) to compress. It is referred to as the universal distortion problem in the literature .\nA natural question to ask is if this class of coding problems can benefit from common randomness .\nRate-distortion-perception Definition (Rate-distortion-perception): For a Polish metric space \\(\\mathcal{X}\\), let \\(( \\mathcal{X}, \\mathbb{B}( \\mathcal{X} ) )\\) be the Borel measurable space induced by the metric. Let \\(\\mathcal{P}( \\mathcal{X} )\\) denote the set of distributions defined over \\(( \\mathcal{X}, \\mathbb{B}( \\mathcal{X} ) )\\), and let \\(X\\) be a random variable with distribution \\(p_X \\in \\mathcal{P}( \\mathcal{X} )\\). Moreover, let \\(\\Delta : \\mathcal{X} \\times \\mathcal{X} \\mapsto [0, \\infty)\\) be a distortion function with \\(\\Delta(x, \\hat{x}) = 0 \\leftrightarrow x = \\hat{x}\\) and let \\(d : \\mathcal{P}( \\mathcal{X} ) \\times \\mathcal{P}( \\mathcal{X} ) \\mapsto [0, \\infty]\\) be a divergence with \\(d( p_X, p_{\\hat{X}} ) = 0 \\leftrightarrow p_X = p_{\\hat{X}}\\). The rate-distortion-perception function for \\(X\\) is given by \\[ R(D, P) = \\inf_{ p_{ \\hat{X}|X } } I(X; \\hat{X}) \\] subject to \\(\\mathbb{E}[ \\Delta(X, \\hat{X}) ] \\leq D\\), \\(d(p_X, p_{ \\hat{X} }) \\leq P\\). Common randomness In this paper , common randomness is useful to reduce the rate of communication required to achieve a certain distortion-perception point even when the randomness is independent of the source. Consider a random source with rate \\(R_0\\) and a code rate of \\(R\\). In essence, there are \\(2^{nR_0}\\) codebooks, each with \\(2^{nR}\\) codes. The common random codeword selects the codebook to use and it can be made to match the distribution of the source. This is interesting and might be useful in MAC and smarthomes .\nHowever, the benefits of common randomness only seems to appear in the perfect realism regime (i.e. \\(d(p_X, p_{ \\hat{X} }) = 0\\)), as this this paper shows. Moreover, it is only benefitial when the definition of perfect realism is one where \\(p_X = p_{\\hat{X}^n}\\). That is, the density divergence is measured jointly rather than single letter. If measured only by additive single letter distortion, then private randomness (i.e., the encoder and decoder both have independent sources of randomness) is sufficient to achieve the optimal rate.\nMultiple-access channel (MAC) TBD\nMultiple-access relay channel (MARC) TBD\nQuantisation Approximate methods Most works consider rounding and using uniform noise to approximate the rounding noise. This has been a massively successful approach; pretty much every paper on compression uses this method. However, this leads to a mismatch between training and test time performance.\nIn terms of novelty, this paper uses the posterior to encode the sample into bits, inspired by arithmetic coding.\nUniversal quantisation In this paper , a universal quantisation scheme is proposed where the problem of communicating a sample \\(z\\) from a noisy channel \\(p(z|x)\\) using as few bits as possible is considered. Theoretically, it is shown that the number of bits to communicate the sample \\(z\\) is not much larger than the mutual information \\(I(x; z)\\) (see paper citation).\nThis quantisation technique is practically implemented in this paper , called relative entropy coding (REC), where an image compression problem is considered using this technique. Relative entropy coding can be defined as follows\nDefinition (Relative entropy coding): Let Q be a target and P be a proposal distribution, with \\(D_{KL}(Q || P) \u003c \\infty\\), and let \\(S = (s_1,s_2, …)\\) be an infinite sequence of publicly available independent fair coin tosses. Relative entropy coding is the problem of producing a uniquely decodable code \\(C\\) representing a sample from \\(Q\\) given \\(S\\), such that the codelength \\(|C|\\) satisfies \\( E[|C|] = \\mathcal{O}(D_{KL}(Q || P)) \\). The key observation is that, in the VAE setup used in that paper, there is a common random distribution, which is the prior of the VAE \\( p(z) \\). Therefore, the amount of information that needs to be sent is only characterised by the KL divergence between the posterior and the prior \\( D(p(z|x) || p(z)) \\). This is highly related to rate-distortion-perception as shown in this paper . The authors show that if it is required that perception (distribution) is perfectly matched, then in some settings, stochastic encoders/decoders with common randomness (e.g., universal quantisation), can achieve lower distortion than deterministic ones.\nAn interesting potential use case for universal quantisation is channel feedback. In a wireless communication channel, the posterior \\( p(z|x) \\) is determined by the channel. Then in order to losslessly send a sample \\( z \\sim p(z|x) \\) from the receiver to the transmitter, REC stipulates that at least \\( \\mathcal{O}(D_{KL}(Q || P)) \\) many bits need to be used. However, the problem is that the receiver only ever observes one sample from the posterior but the universal quantisation method samples from a prior rather than from the posterior. Instead, we can use the hybrid coding scheme, introduced in this paper , where instead of only sending the index, both the quantised sample and the index of the candidate are transmitted. The coding cost of the index is the same while there is an additional cost for transmitting the quantised sample. It is unclear whether this can give good results compared to conventional paradigm of quantising directly.\nA major problem of REC is that without any assumptions on \\(Q\\) and \\(P\\), the runtime of any REC algorithm is \\( \\mathcal{O}( \\exp( D_{\\infty}(Q||P) ) ) \\). However, if \\( dQ/dP \\) (the density ratio) is unimodal, then it can be done in \\( \\mathcal{O}( D_{\\infty}(Q||P) ) \\) (see here ) .\nThe key thing to note about REC is that because the indices have a distribution equal to the posterior, sampling the index is equivalent to sampling from the posterior distribution. Therefore, a code which uniquely identifies the index of a sample can recover the sample exactly. This is a little mind bending. It is somewhat counter intuitive that an integer can have a distribution which is the same as a continuous random variable. But I suppose the key is that the common random randomness is itself continuous, which affords the code to be discrete. Of course, there is also the bias, which never goes away but is negligible if the number of samples from the prior is large enough. The cost is that, for each sample transmitted, a large number of candidates need to be drawn, which is where the \\( \\mathcal{O}( \\exp( D_{\\infty}(Q||P) ) ) \\) runtime complexity comes from.\nAn important theorem that universal quantisation relies on is the strong functional representation (SFR) lemma, which states that for general \\( (X, Y) \\), there exists a \\( Z \\) that is independent of \\( X \\) and \\( Y \\) is a function of \\( (X, Z) \\), such that \\( H(Y | Z) \\leq I(X; Y) + \\log(I(X; Y) + 1) + 4 \\).\nFeedback It is known that feedback does not increase the capacity of a point-to-point channel but can increase reliability.\nPosterior matching Posterior matching is a generalised noiseless feedback scheme that is doubly exponentially capacity achieving for AWGN channels.\nTheorem (Optimal posterior matching): Let \\( \\theta \\) be the message point, \\( P_X \\) the channel input distribution, \\( P_{Y|X} \\) the channel likelihood, and \\( g_n \\) be the transmitter function at time step \\( n \\). Then the optimal posterior matching scheme is defined as \\( g_1(\\theta) = F^{-1}_X (\\theta) \\), \\( g_{n+1}(\\theta | y^n) = (F^{-1}_X \\circ F_{X | Y}(\\cdot | y_n)) \\circ g_n(\\theta | y^{n-1}) \\). The way to think about posterior matching is that the transmitter, observing the noiseless feedback of the receiver’s belief, is trying to send new information so that the receiver is steered to the correct answer.\nInterestingly, there are some similarities between posterior matching and universal quantisation . In posterior matching, the next transmitted random variable \\( X_{n+1} \\) is independent of the previous received values \\( Y^n \\), to ensure that it represents new information. Moreover, the message \\( \\theta \\) can be almost surely uniquely recovered from \\( (X_{n+1}, Y^n) \\) as \\( n \\rightarrow \\infty \\). Therefore, considering the SFR lemma, where it is shown that for general \\( (X, Y) \\), there exists a \\( Z \\) that is independent of \\( X \\), and \\( Y \\) is a function of \\( (X, Z) \\), such that \\( H(Y | Z) \\leq I(X; Y) + \\log(I(X; Y) + 1) + 4 \\). If we map \\( Z \\) to \\( X_{n+1} \\) in the previous problem and \\( Y \\mapsto \\theta \\), \\( X \\mapsto Y^n \\), then we have \\( H(\\theta | X_{n+1}) \\leq I(Y^n; \\theta) + \\log(I(Y^n; \\theta) + 1) + 4 \\). We can interpret \\( H(\\theta | X_{n+1}) \\) as any further uncertainty about the message after transmitting \\( X_{n+1} \\) and \\( I(Y^n; \\theta) \\) as knowledge the receiver has gained already.\n","wordCount":"1566","inLanguage":"en","datePublished":"2023-01-03T00:00:00Z","dateModified":"2023-01-04T12:02:09Z","author":[{"@type":"Person","name":"Tze-Yang Tung"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://r0ymanesco.github.io/my-notes/posts/information_theory/"},"publisher":{"@type":"Organization","name":"My Notebook","logo":{"@type":"ImageObject","url":"https://r0ymanesco.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://r0ymanesco.github.io/my-notes accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://r0ymanesco.github.io/my-notes/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://r0ymanesco.github.io/my-notes/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://r0ymanesco.github.io/my-notes>Home</a>&nbsp;»&nbsp;<a href=https://r0ymanesco.github.io/my-notes/posts/>Topics</a></div><h1 class=post-title>Information Theory</h1><div class=post-meta><span title='2023-01-03 00:00:00 +0000 UTC'>3 January, 2023</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1566 words&nbsp;·&nbsp;Tze-Yang Tung</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#information_theory_a>Rate-distortion</a><ul><li><a href=#information_theory_a1>Universal coding</a></li><li><a href=#information_theory_a2>d-semifaithful coding</a></li></ul></li><li><a href=#information_theory_b>Rate-distortion-perception</a><ul><li><a href=#information_theory_b1>Common randomness</a></li></ul></li><li><a href=#information_theory_c>Multiple-access channel (MAC)</a><ul><li><a href=#information_theory_c1>Multiple-access relay channel (MARC)</a></li></ul></li><li><a href=#information_theory_d>Quantisation</a><ul><li><a href=#information_theory_d1>Approximate methods</a></li><li><a href=#information_theory_d2>Universal quantisation</a></li></ul></li><li><a href=#information_theory_e>Feedback</a><ul><li><a href=#information_theory_e1>Posterior matching</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=information_theory_a>Rate-distortion<a hidden class=anchor aria-hidden=true href=#information_theory_a>#</a></h2><p><em>This is fairly straight forward, will complete later.</em></p><h3 id=information_theory_a1>Universal coding<a hidden class=anchor aria-hidden=true href=#information_theory_a1>#</a></h3><p>Universal coding is concerned with meeting a distortion constraint when the source distribution \(p\) is unknown.
A coding scheme is said to be <em>universal</em> over a class of source distributions if the rate redundancy converges to zero for every source in that class.</p><h3 id=information_theory_a2>d-semifaithful coding<a hidden class=anchor aria-hidden=true href=#information_theory_a2>#</a></h3><p>\(d\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \(d\) of the original source sequence <strong>with probability 1</strong>.
A novel variation of <a href=#information_theory_a1>universal</a>
\(d\)-semifaithful coding is in which the distortion measure is revealed to the encoder alone and only when it receives the source sequence \(x^n\) to compress.
It is referred to as the <em>universal distortion</em> problem in the <a href=https://dx.doi.org/10.1109/ISIT50566.2022.9834395 target=_blank>literature</a>
.</p><p>A natural question to ask is if this class of coding problems can benefit from <a href=#information_theory_b1>common randomness</a>
.</p><h2 id=information_theory_b>Rate-distortion-perception<a hidden class=anchor aria-hidden=true href=#information_theory_b>#</a></h2><ul><li><strong>Definition (Rate-distortion-perception)</strong>:
For a Polish metric space \(\mathcal{X}\), let \(( \mathcal{X}, \mathbb{B}( \mathcal{X} ) )\) be the Borel measurable space induced by the metric.
Let \(\mathcal{P}( \mathcal{X} )\) denote the set of distributions defined over \(( \mathcal{X}, \mathbb{B}( \mathcal{X} ) )\), and let \(X\) be a random variable with distribution \(p_X \in \mathcal{P}( \mathcal{X} )\).
Moreover, let \(\Delta : \mathcal{X} \times \mathcal{X} \mapsto [0, \infty)\) be a distortion function with \(\Delta(x, \hat{x}) = 0 \leftrightarrow x = \hat{x}\) and let \(d : \mathcal{P}( \mathcal{X} ) \times \mathcal{P}( \mathcal{X} ) \mapsto [0, \infty]\) be a divergence with \(d( p_X, p_{\hat{X}} ) = 0 \leftrightarrow p_X = p_{\hat{X}}\).
The rate-distortion-perception function for \(X\) is given by
\[ R(D, P) = \inf_{ p_{ \hat{X}|X } } I(X; \hat{X}) \]
subject to \(\mathbb{E}[ \Delta(X, \hat{X}) ] \leq D\), \(d(p_X, p_{ \hat{X} }) \leq P\).</li></ul><h3 id=information_theory_b1>Common randomness<a hidden class=anchor aria-hidden=true href=#information_theory_b1>#</a></h3><p>In <a href=https://arxiv.org/abs/2202.04147 target=_blank>this paper</a>
, common randomness is useful to reduce the rate of communication required to achieve a certain distortion-perception point even when the randomness is independent of the source.
Consider a random source with rate \(R_0\) and a code rate of \(R\). In essence, there are \(2^{nR_0}\) codebooks, each with \(2^{nR}\) codes.
The common random codeword selects the codebook to use and it can be made to match the distribution of the source.
This is interesting and might be useful in <a href=#information_theory_c>MAC</a>
and <a href=/my-notes/posts/waiveform/#waiveform_c2>smarthomes</a>
.</p><p>However, the benefits of common randomness only seems to appear in the <strong>perfect realism</strong> regime (i.e. \(d(p_X, p_{ \hat{X} }) = 0\)), as this <a href=http://dx.doi.org/10.1109/JSAIT.2022.3231820 target=_blank>this paper</a>
shows.
Moreover, it is only benefitial when the definition of perfect realism is one where \(p_X = p_{\hat{X}^n}\).
That is, the density divergence is measured jointly rather than single letter.
If measured only by additive single letter distortion, then <strong>private randomness</strong> (i.e., the encoder and decoder both have independent sources of randomness) is sufficient to achieve the optimal rate.</p><h2 id=information_theory_c>Multiple-access channel (MAC)<a hidden class=anchor aria-hidden=true href=#information_theory_c>#</a></h2><p><em>TBD</em></p><h3 id=information_theory_c1>Multiple-access relay channel (MARC)<a hidden class=anchor aria-hidden=true href=#information_theory_c1>#</a></h3><p><em>TBD</em></p><h2 id=information_theory_d>Quantisation<a hidden class=anchor aria-hidden=true href=#information_theory_d>#</a></h2><h3 id=information_theory_d1>Approximate methods<a hidden class=anchor aria-hidden=true href=#information_theory_d1>#</a></h3><p>Most works consider rounding and using uniform noise to approximate the rounding noise.
This has been a massively successful approach; pretty much every paper on compression uses this method.
However, this leads to a mismatch between training and test time performance.</p><p>In terms of novelty, this <a href=https://proceedings.mlr.press/v119/yang20a.html target=_blank>paper</a>
uses the posterior to encode the sample into bits, inspired by arithmetic coding.</p><h3 id=information_theory_d2>Universal quantisation<a hidden class=anchor aria-hidden=true href=#information_theory_d2>#</a></h3><p>In this <a href=https://proceedings.neurips.cc/paper/2020/file/92049debbe566ca5782a3045cf300a3c-Paper.pdf target=_blank>paper</a>
, a <em>universal quantisation</em> scheme is proposed where the problem of communicating a sample \(z\) from a noisy channel \(p(z|x)\) using as few bits as possible is considered.
Theoretically, it is shown that the number of bits to communicate the sample \(z\) is not much larger than the mutual information \(I(x; z)\) (see <a href=https://proceedings.neurips.cc/paper/2020/file/92049debbe566ca5782a3045cf300a3c-Paper.pdf target=_blank>paper</a>
citation).</p><p>This quantisation technique is practically implemented in this <a href=https://arxiv.org/abs/2010.01185 target=_blank>paper</a>
, called relative entropy coding (REC), where an image compression problem is considered using this technique.
Relative entropy coding can be defined as follows</p><ul><li><strong>Definition (Relative entropy coding)</strong>:
Let Q be a target and P be a proposal distribution, with \(D_{KL}(Q || P) &lt; \infty\), and let \(S = (s_1,s_2, &mldr;)\) be an infinite sequence of publicly available independent fair coin tosses.
Relative entropy coding is the problem of producing a uniquely decodable code \(C\) representing a sample from \(Q\) given \(S\), such that the codelength \(|C|\) satisfies \( E[|C|] = \mathcal{O}(D_{KL}(Q || P)) \).</li></ul><p>The key observation is that, in the VAE setup used in that paper, there is a common random distribution, which is the prior of the VAE \( p(z) \).
Therefore, the amount of information that needs to be sent is only characterised by the KL divergence between the posterior and the prior \( D(p(z|x) || p(z)) \).
This is highly related to <a href=#information_theory_b>rate-distortion-perception</a>
as shown in this <a href=https://arxiv.org/abs/2102.09270 target=_blank>paper</a>
.
The authors show that if it is required that perception (distribution) is perfectly matched, then in some settings, stochastic encoders/decoders with <a href=#information_theory_b1>common randomness</a>
(e.g., universal quantisation), can achieve lower <strong>distortion</strong> than deterministic ones.</p><p>An interesting potential use case for universal quantisation is <strong>channel feedback</strong>.
In a wireless communication channel, the posterior \( p(z|x) \) is determined by the channel.
Then in order to losslessly send a sample \( z \sim p(z|x) \) from the receiver to the transmitter,
REC stipulates that at least \( \mathcal{O}(D_{KL}(Q || P)) \) many bits need to be used.
However, the problem is that the receiver only ever observes one sample from the posterior but the universal quantisation method samples from a prior rather than from the posterior.
Instead, we can use the hybrid coding scheme, introduced in this <a href=https://proceedings.mlr.press/v162/theis22a.html target=_blank>paper</a>
, where instead of only sending the index, both the <strong>quantised</strong> sample and the index of the candidate are transmitted.
The coding cost of the index is the same while there is an additional cost for transmitting the quantised sample.
It is unclear whether this can give good results compared to conventional paradigm of quantising directly.</p><p>A major problem of REC is that without any assumptions on \(Q\) and \(P\), the runtime of any REC algorithm is \( \mathcal{O}( \exp( D_{\infty}(Q||P) ) ) \).
However, if \( dQ/dP \) (the density ratio) is unimodal, then it can be done in \( \mathcal{O}( D_{\infty}(Q||P) ) \) (see <a href=https://arxiv.org/abs/2010.01185 target=_blank>here</a>
) .</p><p>The key thing to note about REC is that because the indices have a distribution equal to the posterior, sampling the index is equivalent to sampling from the posterior distribution.
Therefore, a code which uniquely identifies the index of a sample can recover the sample exactly.
This is a little mind bending. It is somewhat counter intuitive that an integer can have a distribution which is the same as a continuous random variable.
But I suppose the key is that the common random randomness is itself continuous, which affords the code to be discrete.
Of course, there is also the bias, which never goes away but is negligible if the number of samples from the prior is large enough.
The cost is that, for each sample transmitted, a large number of candidates need to be drawn, which is where the \( \mathcal{O}( \exp( D_{\infty}(Q||P) ) ) \) runtime complexity comes from.</p><p>An important theorem that universal quantisation relies on is the strong functional representation <a href=http://dx.doi.org/10.1109/TIT.2018.2865570 target=_blank>(SFR)</a>
lemma, which states that for general \( (X, Y) \), there exists a \( Z \) that is independent of \( X \) and \( Y \) is a function of \( (X, Z) \), such that \( H(Y | Z) \leq I(X; Y) + \log(I(X; Y) + 1) + 4 \).</p><h2 id=information_theory_e>Feedback<a hidden class=anchor aria-hidden=true href=#information_theory_e>#</a></h2><p>It is known that feedback does not increase the capacity of a point-to-point channel but can increase reliability.</p><h3 id=information_theory_e1>Posterior matching<a hidden class=anchor aria-hidden=true href=#information_theory_e1>#</a></h3><p><a href=http://dx.doi.org/10.1109/TIT.2011.2104992 target=_blank>Posterior matching</a>
is a generalised noiseless feedback scheme that is doubly exponentially capacity achieving for AWGN channels.</p><ul><li><strong>Theorem (Optimal posterior matching)</strong>:
Let \( \theta \) be the message point, \( P_X \) the channel input distribution, \( P_{Y|X} \) the channel likelihood, and \( g_n \) be the transmitter function at time step \( n \).
Then the optimal posterior matching scheme is defined as \( g_1(\theta) = F^{-1}_X (\theta) \), \( g_{n+1}(\theta | y^n) = (F^{-1}_X \circ F_{X | Y}(\cdot | y_n)) \circ g_n(\theta | y^{n-1}) \).</li></ul><p>The way to think about posterior matching is that the transmitter, observing the noiseless feedback of the receiver&rsquo;s belief, is trying to send new information so that the receiver is steered to the correct answer.</p><p>Interestingly, there are some similarities between posterior matching and <a href=#information_theory_d2>universal quantisation</a>
.
In posterior matching, the next transmitted random variable \( X_{n+1} \) is independent of the previous received values \( Y^n \), to ensure that it represents new information.
Moreover, the message \( \theta \) can be almost surely uniquely recovered from \( (X_{n+1}, Y^n) \) as \( n \rightarrow \infty \).
Therefore, considering the <a href=http://dx.doi.org/10.1109/TIT.2018.2865570 target=_blank>SFR</a>
lemma, where it is shown that for general \( (X, Y) \), there exists a \( Z \) that is independent of \( X \), and \( Y \) is a function of \( (X, Z) \), such that \( H(Y | Z) \leq I(X; Y) + \log(I(X; Y) + 1) + 4 \).
If we map \( Z \) to \( X_{n+1} \) in the previous problem and \( Y \mapsto \theta \), \( X \mapsto Y^n \), then we have \( H(\theta | X_{n+1}) \leq I(Y^n; \theta) + \log(I(Y^n; \theta) + 1) + 4 \).
We can interpret \( H(\theta | X_{n+1}) \) as any further uncertainty about the message after transmitting \( X_{n+1} \) and \( I(Y^n; \theta) \) as knowledge the receiver has gained already.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://r0ymanesco.github.io/my-notes/tags/readinglist/>ReadingList</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://r0ymanesco.github.io/my-notes>My Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>