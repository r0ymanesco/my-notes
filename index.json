[{"content":"Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.\nTo solve these problems, the new approach will split the problem into 3 sub-problems:\nUser management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once. We also want to reduce user dependency on the libraries installed on the servers themselves in order to make software updates easier. Finally, in order to support the skeleton approach to server libraries, we will use Docker to launch jobs on a specific machine of the user\u0026rsquo;s choice. The dashboard currently being used to monitor GPU utilisation can continue to exist so that users know which machines are free to launch jobs to. The next section outlines how these objectives will be accompolished.\nSolution Firstly, to facilitate this transition, users should be notified of a timeline for doing so and server managers should present a timetable for how this transition will be carried out. I suggest migrating servers in batches to this new architecture to reduce disruption. Users should be asked to backup their data.\nUser management In order to centralise user management, we will use the LDAP system available on all Linux distributions. The system uses a host server to store user login information where client servers can authenticate against. This way, new users only need to be added to the host server. Any machine can be the host but I recommend using denizpower as it already hosts the dashboard.\nFor storage, we will take all of the hard drives that are in each individual server and integrate them into a network drive (e.g., NAS or just attach them to the LDAP host server). Every user would store their data in this large hard drive so that we don\u0026rsquo;t run out of space on the system drive in the client servers.\nLibrary management Previously, due to the wide variety of Linux distributions in the cluster, each machine had different versions of the kernel, NVIDIA drivers, \u0026hellip;etc, which is extremely painful to update. In fact, for all intents and purposes, we did not update machines at all after the initial installation.\nTo make this process easier, we will instead only install the minimum amount of libraries on each server and rely on Docker to deploy jobs with specific library requirements (see below ). The minimum software requirement on each server are the following:\nBase Linux distribution (I recommend using Archlinux because it has very comprehensive documentation for LDAP and Docker). Docker (make sure the service is enabled after installation) NVIDIA drivers (might not be needed, see below ) Job deployment As mentioned above, we will use Docker to deploy jobs. Docker is a containerised application that allows you to specify the code and required libraries to run the code, all completely independent from the base OS installation. This means that each user can simply build containers with the libraries that they need without having to install any of the libraries on the base OS itself. The lab can also create containers with some basic libraries, like specific Pytorch/Tensorflow versions, that others can use as the base and add additional libraries as required. These containers can be hosted on Docker Hub for everyone to download. You can also specify in your container to copy the necessary files from the central storage to the client server and remove it when the job is done so that the client drive doesn\u0026rsquo;t run out of space. It may even be possible to have the NVIDIA drivers installed within the Docker container itself (see here ), but it is not well documented and might require some experimentation. Users would check on the dashboard to see which server is available and deploy jobs to the selected server.\nDocker does require some effort to learn and get used to but in the long run, it will be worth it. It\u0026rsquo;s also a useful skill in industry as many tech companies use Docker to run jobs on their servers.\nNew servers As Deniz mentioned, we will be purchasing new servers soon. As someone that has gone through the purchasing process of many of the servers you see on the dashboard, I want to give my opinion on how I configure the hardware.\nFirstly, the person to contact about purchasing new servers is Tim Brown (email: Tim.Brown@pcspecialist.co.uk ). He is our contact at PC Specialist, which is Imperial\u0026rsquo;s contracted company for computer equipment purchases. You can go to PC Specialist\u0026rsquo;s website to configure a machine and send it to Tim for a quote. You can usually get a discount on the price.\nSecondly, try to put as many GPUs in a single machine as possible. This is because our jobs are not very CPU intensive so it doesn\u0026rsquo;t make sense to only put 1 or 2 GPUs in a machine. Each machine also takes up a considerable amount of space on the server rack so we want to be as space efficient as possible. However, when you configure a machine on PC Specialist\u0026rsquo;s website, they may not let you put more than a certain number of GPUs in a machine. What you should do in that case is configure the maximum allowed by the website and then contact Tim to ask him what the maximum number allowed actually is. Often they will have other chassis available that can hold more GPUs.\nLastly, when striking a balance between GPU compute, memory and price, consider the use case first. So far, we have only purchased gaming grade GPUs, which offer at most 24GB memory and moderate performance. This is because they are very affordable for what they offer, and our computing needs have not been so extreme that these GPUs cannot handle them. Moreover, due to the low price, we can buy many of them so that we have a large number of GPUs for everyone to use.\nHowever, recently there has been increasing demand for compute as some people have begun to train reasonably large Transformer models. Therefore, it might make sense to purchase a few professional grade GPUs, like the NVIDIA RTX 6000 (48GB) . Bare in mind that such GPUs are extremely expensive and can itself cost as much as a single server that we have currently.\nAt the end of the day, it comes down to the budget and use cases. If you want to ask me questions, you can reach me at my email: tzeyang.tung.work@gmail.com ","permalink":"https://r0ymanesco.github.io/my-notes/projects/server_migration/","summary":"Architecture The current architecture of independent servers with a dashboard for monitoring usage is unsustainable. As the number of servers have increased, adding new users becomes cumbersome as each machine needs to be individually added. Moreover, each server runs different versions of Linux and drivers, making software updates extremely difficult.\nTo solve these problems, the new approach will split the problem into 3 sub-problems:\nUser management Library management Job deployment The central idea is to have a single server that manages logins for every other machine in the cluster so that each new user only needs to be added once.","title":"Server Migration"},{"content":"Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:\nThe implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train. It would make more sense to try to improve the current implementation.\nNew encode/decode scheme What transformers are really good at is autoregression. So the question we want to pose to a transformer is: \u0026ldquo;given the prior tokens, what is the next token?\u0026rdquo;\nGiven frames \\( \\{x_1, \u0026hellip;, x_n\\} \\), with corresponding features \\( y_i = E(x_i) \\).\nEncoder: learn a \u0026ldquo;query\u0026rdquo; token \\( Q_z \\) that learns the codeword given a sequence of frame features \\( \\{y_1,\u0026hellip;,y_n\\} \\). That is, we are modelling \\( P(z_n | y_1, \u0026hellip;, y_n) \\) by querying the self-attention output of \\( \\{y_1,\u0026hellip;,y_n\\} \\). This can be done sequentially as frames arrive and we can observe the entropy of \\( z_n \\) to determine the GoP cut-off. Causality is naturally preserved. Note that this is NOT an autoregressive process. If we want the codeword to be autoregressive, we can condition on \\( (y_i, z_{n-1}) \\). Decoder: given \\( z_n \\), learn a \u0026ldquo;start decode\u0026rdquo; token \\( \\hat{y}_0 \\), such that we autoregressively decode the frame features. That is, we are modelling \\( P(\\hat{y}_n | \\hat{y}_0,\u0026hellip;, \\hat{y}_{n-1}; z_n) \\). The estimated frame can then be obtained as \\( \\hat{x}_n = D(\\hat{y}_n) \\). To ensure causality, the decoding should be masked. Unfortunately the new design still does not work. The next step is to try an autoregressive encoder. The encoding will be slower due to the iterative nature of the code but it may be better due to the progressiveness rather than single step coding.\nAnother option is to implement the same idea but using 2D transformers. This is easier so will go first.\nConnection to R-D Given a channel bandwidth constraint, you could in theory traverse the R-D trade-off by considering, given a target distortion, how many frames can I encode into the available channel resources? The distortion would be lower-bounded by the distortion offered by encoding 1 frame. More frames encoded implies lower rate (per frame) and therefore higher distortion. This is in contrast to typical R-D trade-off, as this implies that greater latency (encoding more frames) increases distortion.\nSnapshot compressive imaging (SCI) The STformer seems to be a good way to perform 2D transformer. Moreover, the snapshot compressive imaging (SCI) problem, sounds very interesting and may be applicable to our problem. In the SCI formulation, the encoding is done via the summation of frames, each multiplied with a mask. Then given the mask, and the result of the summation, the decoder tries to invert the process. This is akin to an under-determined inverse problem. If we use this for our coding problem, this would essentially entail a simple encoder, where each frame is mapped to a feature and those features are summed together to form the codeword. Then the decoder, knowing the filters used in the encoding process, inverts the summation and reproduces the original frames.\nSCI is basically an inverse problem (very similar to compressive sensing). We can use a transformer to learn the masks for each feature tensor and sum them up. The receiver then computes the inverse problem given the \u0026ldquo;measurements\u0026rdquo; (masks). If we want to achieve autoregressive coding, we can just condition the next mask on the previous masks. The decoder can be like the STformer This idea has been shelved for now as it requires the masks to be sent separately.\nVCT based R-D prediction We can use the same model as VCT to predict the distribution and use it to determine the number of channel uses. Then a second transformer would predict the image/video quality instead of the distribution. Once both transformers are trained, we can traverse the R-D curve freely. That is, we first map frames into the latent space via an encoder \\( z_t = E(x_t) \\). Then, we want to estimate \\( Q(z_t) = f_\\theta(z_{t-1},\u0026hellip;,z_0) \\), where \\( Q \\) is the quality function, parameterised by \\( \\theta \\).\nTo generate codewords, we condition the current frame\u0026rsquo;s latent vector on the previous frames\u0026rsquo; latent vectors \\( c_t = f(z_t | z_{t-1},\u0026hellip;,z_0) \\), using a transformer.\nCurrently, the coding stage uses reconstructions from the receiver at a random rate to infer the current frame. This might lead to suboptimal results as initial reconstructions are poor, therefore affecting the reconstructions of future frames. However, accumulating gradients across the entire video sequence would require too much memory. Perhaps there is a way to do curriculum training, starting with feature encoder only before jointly training with the transformer, to find a middle ground.\nIn order to preserve the ordering of rate-distortion curve, the output of the predictor is a cumulative sum. The predictor training using a transformer conditioned on prior frames does indeed work. The question is whether the accuracy is good enough to determine the rate. This result is also based on codewords which are the output of the feature encoder, without prior frame conditioning.\nCompression quality prediction The same concept can be used to predict the compression quality for a given number of bits. The VCT paper only considers average quality for a given number of bits. If we allow variable rate compression, then we can instead target a fixed quality and adjust the bit-rate to obtain it. The result would be for a chosen distortion, an average bit-rate to achieve it.\nArchitecture According to the VCT video compression paper, it is possible to train jointly with this architecture using the architecture in ELIC . Preliminary results seems to suggest a pretty substantial improvement using this architecture, over the one proposed by VCT. The primary difference is that the GDN layers are replaced by residual layers. It is interesting to see that the GDN layer is becoming out of fashion.\nOn the other hand, the latent residual predictor proposed by the VCT paper was not useful for our case since the codeword and the decoder both already condition on the previous frames as well as the current frame tokens.\nHyperparameters In the ELIC paper, they suggest using dynamic weights on the loss of different rates to improve the generalisation for each rate. There are two approaches:\nThe idea is that, let \\( D_r \\) be the average distortion of the model at rate \\( r \\), then if the average improves by more than a constant factor \\( \\delta \\), then the weighting \\( \\mu_r \\) at that rate will decrease, and vice versa. The loss is then \\( \\sum_r \\mu_r D_r \\). Another approach is to compare frames at successive rates and ensure that the improvement gap between each successive rate is sufficiently large. The paper actually suggests using a different rate for each delay (time instance from the last key frame), but right now we are just averaging over time and using a single weight for each rate. Although the results suggest the first approach works better, it seems that the weighting needs to be normalised as the gradients become unstable if the weights are too large. In the first method, the gradients became unstable after \\( \\mu_r \u0026gt; 8 \\) but could be much lower in theory as the second method became unstable after \\( \\mu_r \u0026gt; 4 \\). This is not surprising as the weighting scales the gradient.\nAblation study In terms of how codewords are generated, there are two methods:\nWe can simply take the latent vector of each frame \\( c_t = z_t \\), and use the predictor \\( Q \\) to determine how many tokens to transmit. We can obtain the codeword conditioned on the previous frames \\( c_t = f(z_t | z_{t-1},\u0026hellip;,z_0) \\). The results of this study showed that the conditional codewords perform better, likely because the encoder can learn to discard unnecessary information if it knows what has already been transmitted.\nIn terms of the predictor:\nTake the conditional codewords and use a MLP to predict the quality of each rate. Use the input of the conditional codeword transformer and use a separate transformer to predict the quality at each rate. Use the output of the conditional codeword transformer and run a second transformer on those codewords conditioned on the prior codewords. Both the second and third methods work well. To reduce the memory usage during training, the second method is preferred.\nIn terms of references for decoding, the following two approaches are considered:\nUsing the decoded frames to train a separate auto-feature encoder Use the noisy codewords from the prior transmission directly Likewise, for encoding, the encoder can either use zeros to initialize the prior frame latent conditioning, or use learned features obtained from mapping all zero frames to the latent. The second approach uses less memory and is equally good.\nBandwidth allocation Having obtained a predictor/entropy model of the tokens, we would now like to allocate bandwidth according to this information. Chenghong has a method where we input the bandwidth ratio into a MLP along with the tokens and that allows the MLP to learn to allocate bandwidth accordingly. However, we would like the bandwidth input to also be a learn-able parameter. The point is to avoid ordered information, as currently the predictor/entropy model tells us how many tokens are needed to achieve a certain quality if the tokens were transmitted sequentially. This has proven to be a performance degrader, compared to unordered encoding.\n","permalink":"https://r0ymanesco.github.io/my-notes/projects/deepjscc_stream/","summary":"Using transformers to autoregressively predict the next codeword Poor results The encoding/decoding approach used so far has not worked:\nThe implementation that uses 2D transformers eventually diverged and crashed. The impelmentation using VTC transformers stopped improving and produced bad results, weaker than the 2D transformers model. However, the VCT approach seems to be more promising as the transformer video compression paper (VCT, Mentzer) has shown it can work. It is also much faster to train.","title":"DeepJSCC Stream"},{"content":"Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.\nApplications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample. Although compression using diffusion models has been demonstrated already , their approach relies on a latent variable with which the decoder is conditioned on, in order to guide decoder. However, this is a tall ask for a single, one step, latent variable. Typically, diffusion models will condition on all of the latents in each step, whereas here they are only conditioning on a single variable generated in one step. This single variable is then quantised and coded as many prior compression works have used, e.g., uniform noise to approximate rounding error and a hyperprior density estimation.\nA better approach would be to transmit those latent vectors in each step using universal quantisation. Starting from the highest step, where the posterior is almost an isotropic Gaussian distribution, we send a sample from this distribution using universal quantisation and the decoder tries to approximate likelihood function. \\( \\forall t, x_{t-1} \\Rightarrow q(x_t | x_{t-1}) \\stackrel{x_t}{\\Rightarrow} p(x_{t-1} | x_t) \\Rightarrow \\hat{x}_{t-1} \\), compute VAE loss \\( l_{ELBO}(x_{t-1}, \\hat{x}_{t-1}) \\). Moreover, the coding rate should be approximately constant since at each step \\( q(x_t | x_{t-1}) \\approx p(x_{t-1} | x_t) \\), therefore the KL divergence should be small. Essentially, the cost of coding is amortised into the diffusion steps. Even though this doesn\u0026rsquo;t make the coding speed any faster, it does provide a way to make rate-distortion trade-offs since we can stop coding after a certain point and obtain the intermediate image.\nAnother application is to consider a stochastic process which is conditional on a common random variable and the encoder wishes to communicate a sample of the stochastic process to a decoder. This could be an interesting goal-oriented communication problem .\nAnother interesting possibility is that because both the sender and the receiver are required to have a common random source, could the seed be used as a secret key for symmetric encryption? Basically, without the common random source, is it possible to decode successfully, even if approximately? (might be hard to prove the security of the scheme although maybe some kind of privacy guarantees can be made)\nAnother possibility is \u0026lt;strong\u0026gt;posterior matching\u0026lt;/strong\u0026gt; . Interestingly, there are some similarities between posterior matching and universal quantisation . In posterior matching, the next transmitted random variable \\( X_{n+1} \\) is independent of the previous received values \\( Y^n \\), to ensure that it represents new information. Moreover, the message \\( \\theta \\) can be almost surely uniquely recovered from \\( (X_{n+1}, Y^n) \\) as \\( n \\rightarrow \\infty \\). An important theorem that universal quantisation relies on is the strong functional representation (SFR) lemma, which states that for general \\( (X, Y) \\), there exists a \\( Z \\) that is independent of \\( X \\) and \\( Y \\), and is a function of \\( (X, Z) \\), such that \\( H(Y | Z) \\leq I(X; Y) + \\log(I(X; Y) + 1) + 4 \\). Therefore, if we map \\( Z \\) to \\( X_{n+1} \\) in the previous problem and \\( Y \\mapsto \\theta \\), \\( X \\mapsto Y^n \\), then we have \\( H(\\theta | X_{n+1}) \\leq I(Y^n; \\theta) + \\log(I(Y^n; \\theta) + 1) + 4 \\). We can interpret \\( H(\\theta | X_{n+1}) \\) as any further uncertainty about the message after transmitting \\( X_{n+1} \\) and \\( I(Y^n; \\theta) \\) as knowledge the receiver has gained already.\nWe can apply universal quantisation to the transmission of a stochastic policy to a remote agent for MDPs or POMDPs . That is, consider a controller which has policy \\( \\pi(a | s) \\), and wishes to transmit a random action \\( a \\sim \\pi(a | s) \\) to a remote agent to carry out. It is trivial to see that this corresponds exactly to the SFR lemma, where \\( Y \\mapsto a \\) and \\( X \\mapsto s \\). It corresponds to the \u0026ldquo;sending actions\u0026rdquo; policy we have considered in the past. A few scenarios to consider:\nIf we have an exact channel model \\( P(\\hat{a} | a) \\), is there a JSCC equivalent of this? What if we have noiseless feedback of \\( \\hat{a} \\)? What if the controller also needs to learn the optimal policy? In this case, it is possible that initially, we would communicate very little so that the remote agent simply takes random actions based on a prior distribution. As the controller learns based on the reward feedback of the agent\u0026rsquo;s actions, it then begins to communicate what it thinks is optimal. This can be done using the common randomness (prior). The agent can construct an empirical distribution of the policy over time and once it has a sufficiently good estimate, no more communication is required. In this paper , a similar problem considering contexual multi-armed bandits (CMAB) was solved. If we have a MA-POMDP, the problem can be seen as matching the joint distribution of the agents\u0026rsquo; actions with the optimal policy over all the agents. Common randomness could also be useful in this case to reduce the communication load and could perhaps even be done with simply a correlated random variable . What if the action space is continuous? Universal quantisation could be used to quantise this to a finite rate? Can this be applied to a goal-oriented quantisation problem? This might be used in a sensor network. MA-POMDP Problem Consider a 2-agent POMDP , where the environment is governed by the transition kernel \\( P(s_1^t, s_2^t | a_1^{t-1}, a_2^{t-1}) \\) and the agents have policies \\( \\pi_i^t(a_i^t | s_i^t, m_i^t)\\; i \\in \\{0, 1\\} \\). Let\u0026rsquo;s assume that the union of the observations is in the state \\( s_1 \\cup s_2 \\in \\mathcal{S} \\). The messages \\( m_i^t \\) are generated by the other agent and sent to the opposite agent. We want to determine the minimum rate of the messages such that the agents converge to the optimal policy \\( \\pi^\\ast(a_1^t, a_2^t | s_1^t, s_2^t) \\).\nInitial thoughts It is valid to first consider what are the missing information for each agent to obtain stationarity. For each agent, they need to be able to know the policy and observation of the other agent in order to have the complete information necessary to observe a stationary POMDP. From the perspective of agent 1, if we know the transition kernel \\( P(s_1^t, s_2^t | a_1^{t-1}, a_2^{t-1}) \\), then if we know the action taken by agent 2, we can code the observation of agent 2 using \\( P(s_2^t | a_1^{t-1}, a_2^{t-1}) \\) by marginalising over \\( s_1^t \\). In order to send the action, if we are given common randomness, then we can initialise both policies using the same random variable so that at \\( t = 1 \\), each agent simply codes the action taken based on \\( \\pi_i^1(a_i^1) \\). For \\( t \u0026gt; 1 \\), instead of coding the action based on \\( \\pi_i^t(a_i^t | s_i^t) \\), we can use relative entropy coding to send a sample \\( a_i^t \\sim \\pi_i^t(a_i^t | s_i^t) \\) using \\( 2^\\epsilon \\) samples and \\( \\epsilon \\) bits, where \\( \\epsilon = D_{KL}(\\pi_i^t || \\pi_i^{t-1}) \\). Moreover, if we use a update policy like trust region policy optimisation (TRPO), then it is guaranteed that \\( D_{KL}(\\pi_i^t || \\pi_i^{t-1}) \\leq \\epsilon \\). Knowing the action and the observation of the other agent, the agent can update its belief of the other agent\u0026rsquo;s policy exactly the same as the other agent. Since the belief matches the policy exactly, \\( \\pi_i^t \\) becomes the new common randomness. Therefore, in each round, the agents only need to send the amount of update that their policies changed by. In fact, if we use something similar to epsilon greedy for exploration, then the prior chosen at the beginning would represent the exploration distribution and while epsilon is large at the beginning, fewer communication would be needed, and as epsilon gets smaller we would communicate more.\nThe thing that needs to be checked is whether or not no common randomness can achieve the same thing. If no common prior on the policy is given at \\( t = 1 \\), then each agent can only form a belief that may not be the same as the policy of the other agent (private randomness). Therefore, to send the action, the agent must use at least \\( H(a_i^t | s_i^t) \\) bits, based on the current policy \\( \\pi_i^t \\). This can be significantly bigger than \\( \\epsilon \\). The difference would be even greater if the action space was continuous since the private randomness version would need to quantize, leading to loss in performance.\nThis project also merges well with deep RL since the agent policies can be parameterized by DNNs.\n","permalink":"https://r0ymanesco.github.io/my-notes/projects/goal_oriented_coding/","summary":"Project goal This project aims to leverage universal quantisation for goal-oriented coding. It is closely related to the rate-distortion-perception , as it was shown that under perfect perceptual constraint, a stochastic encoder/decoder can improve the distortion.\nApplications A potential application for universal quantisation in compression is in diffusion models. Diffusion models essentially compute the VAE objective for every time step (usually using randomly chosen time step) to guide the decoder from isotropic noise to an image sample.","title":"Goal-oriented Coding"},{"content":"Graph Coding Rewrite paper from the perspective of source coding and planning Motivate each solution as a special cost structure of a more general definition Look into source coding of interesting sources, for example, AoI source coding. Then highlight the differences. Goal oriented compression (source coding) Most of these works are still considering single letter reconstruction. We are considering reconstruction over multiple time steps, or equivalently, reconstruction cost minimization. This is more general since the distortion/cost is more arbitrary. The goal-oriented compression section in Deniz paper seems particularly relevant. It is related to remote source coding problem. Can we construct a cost structure where acknowledgement is required so that we can formulate the whole problem as a cost minimizing source coding?\nClassical planning problems The objective in classical path planning problems is to find a plan: a sequence of actions mapping from an initial state to a goal state, in a state space. The search can be constrained, such as specifying that the plan must pass through certain states or optimized such that the plan contains the fewest actions. The challenge is due to the combinatorial nature of such problems. Typically heuristics are used to reduce the search space. This is similar to semantics as heuristics can be considered the semantics to be exploited to make communication easier. In our case, the heuristics are paths and we want to communicate only the path.\nOptimising the communication tree The solutions we have constructed so far for the path coding problem are based on heuristics and suboptimal. Even the dynamic shortest path coding is suboptimal because it depends on the optimality of the structure of the tree. Specifically, the challenge lies in finding a tree structure such that the average cost to reach the goal state is lowest. On the surface, this seems to be a combinatorial search problem, but a few ideas may help reduce the complexity of the search.\nOptimal binary search trees The paper \u0026ldquo; optimum binary search trees \u0026rdquo; describes a similar problem to the one we have. The problem therein is to construct a binary search tree where the nodes do not have the same probability. They describe an algorithm which can run in \\( O(n^2) \\) time.\nIs it possible to turn this problem into ours? In the paper, it is said that finding the optimum search tree is equivalent to the problem of \u0026ldquo;finding a binary tree of minimum weighted path legnth, with the weights applied from left to right in the tree\u0026rdquo;.\nIs this is also true for a m-ary tree? If so, this might translate to our problem, with \\( m = |\\mathcal{C}_t| \\) (the rate of communication).\n","permalink":"https://r0ymanesco.github.io/my-notes/projects/graph_coding/","summary":"Graph Coding Rewrite paper from the perspective of source coding and planning Motivate each solution as a special cost structure of a more general definition Look into source coding of interesting sources, for example, AoI source coding. Then highlight the differences. Goal oriented compression (source coding) Most of these works are still considering single letter reconstruction. We are considering reconstruction over multiple time steps, or equivalently, reconstruction cost minimization. This is more general since the distortion/cost is more arbitrary.","title":"Graph Coding"},{"content":"Move away from GNURadio GNURadio is too hard to maintain. It has too many dependencies and is hard to write modules for. It would be better to simplify to just a Python script with SoapySDR interfaces.\nRust implementation The SoapySDR C++ library has Rust bindings, which can be used to set the LimeSDR parameters. Rust should provide much better efficiency compared to Python, although Python might be easier to for experimentation. A small proof-of-concept project that uses the Rust bindings to send a singletone sinusoid is completed .\n","permalink":"https://r0ymanesco.github.io/my-notes/projects/lime_sdr/","summary":"Move away from GNURadio GNURadio is too hard to maintain. It has too many dependencies and is hard to write modules for. It would be better to simplify to just a Python script with SoapySDR interfaces.\nRust implementation The SoapySDR C++ library has Rust bindings, which can be used to set the LimeSDR parameters. Rust should provide much better efficiency compared to Python, although Python might be easier to for experimentation.","title":"Lime SDR"},{"content":"Markov decision processes (MDP) Definition TBD Model based Partially observable Markov decision processes (POMDP) Definition TBD There is a surprising amount of convergence properties for POMDPs.\n","permalink":"https://r0ymanesco.github.io/my-notes/posts/reinforcement_learning/","summary":"Markov decision processes (MDP) Definition TBD Model based Partially observable Markov decision processes (POMDP) Definition TBD There is a surprising amount of convergence properties for POMDPs.","title":"Reinforcement Learning"},{"content":"Rate distortion interpretation Rate-distortion theory is described in detail here . An example of this can be seen in the project DeepJSCC Stream . Specifically, the approach where a conditional transformer is used to predict the number of channel uses needed to achieve a certain reconstruction quality is exactly a rate-distortion problem.\nRelationship to text-to-image generators There seems to be a connection between semantics and text-to-image generators. Clearly the semantic of the text and image are being matched to generate images that match the description of the text.\nGoal-oriented communications Goal oriented communication can best be described as a remote rate distortion problem . The idea is that a feature of the source is observed through only the source and the receiver\u0026rsquo;s job is to reconstruct the feature, rather than the source. In this case, the goal is the feature.\n","permalink":"https://r0ymanesco.github.io/my-notes/posts/semantic_communications/","summary":"Rate distortion interpretation Rate-distortion theory is described in detail here . An example of this can be seen in the project DeepJSCC Stream . Specifically, the approach where a conditional transformer is used to predict the number of channel uses needed to achieve a certain reconstruction quality is exactly a rate-distortion problem.\nRelationship to text-to-image generators There seems to be a connection between semantics and text-to-image generators. Clearly the semantic of the text and image are being matched to generate images that match the description of the text.","title":"Semantic Communications"},{"content":"Rate-distortion This is fairly straight forward, will complete later.\nUniversal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \\(p\\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.\nd-semifaithful coding \\(d\\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \\(d\\) of the original source sequence with probability 1. A novel variation of universal \\(d\\)-semifaithful coding is in which the distortion measure is revealed to the encoder alone and only when it receives the source sequence \\(x^n\\) to compress. It is referred to as the universal distortion problem in the literature .\nA natural question to ask is if this class of coding problems can benefit from common randomness .\nRate-distortion-perception Definition (Rate-distortion-perception): For a Polish metric space \\(\\mathcal{X}\\), let \\(( \\mathcal{X}, \\mathbb{B}( \\mathcal{X} ) )\\) be the Borel measurable space induced by the metric. Let \\(\\mathcal{P}( \\mathcal{X} )\\) denote the set of distributions defined over \\(( \\mathcal{X}, \\mathbb{B}( \\mathcal{X} ) )\\), and let \\(X\\) be a random variable with distribution \\(p_X \\in \\mathcal{P}( \\mathcal{X} )\\). Moreover, let \\(\\Delta : \\mathcal{X} \\times \\mathcal{X} \\mapsto [0, \\infty)\\) be a distortion function with \\(\\Delta(x, \\hat{x}) = 0 \\leftrightarrow x = \\hat{x}\\) and let \\(d : \\mathcal{P}( \\mathcal{X} ) \\times \\mathcal{P}( \\mathcal{X} ) \\mapsto [0, \\infty]\\) be a divergence with \\(d( p_X, p_{\\hat{X}} ) = 0 \\leftrightarrow p_X = p_{\\hat{X}}\\). The rate-distortion-perception function for \\(X\\) is given by \\[ R(D, P) = \\inf_{ p_{ \\hat{X}|X } } I(X; \\hat{X}) \\] subject to \\(\\mathbb{E}[ \\Delta(X, \\hat{X}) ] \\leq D\\), \\(d(p_X, p_{ \\hat{X} }) \\leq P\\). Common randomness In this paper , common randomness is useful to reduce the rate of communication required to achieve a certain distortion-perception point even when the randomness is independent of the source. Consider a random source with rate \\(R_0\\) and a code rate of \\(R\\). In essence, there are \\(2^{nR_0}\\) codebooks, each with \\(2^{nR}\\) codes. The common random codeword selects the codebook to use and it can be made to match the distribution of the source. This is interesting and might be useful in MAC and smarthomes .\nHowever, the benefits of common randomness only seems to appear in the perfect realism regime (i.e. \\(d(p_X, p_{ \\hat{X} }) = 0\\)), as this this paper shows. Moreover, it is only benefitial when the definition of perfect realism is one where \\(p_X = p_{\\hat{X}^n}\\). That is, the density divergence is measured jointly rather than single letter. If measured only by additive single letter distortion, then private randomness (i.e., the encoder and decoder both have independent sources of randomness) is sufficient to achieve the optimal rate.\nMultiple-access channel (MAC) TBD\nMultiple-access relay channel (MARC) TBD\nQuantisation Approximate methods Most works consider rounding and using uniform noise to approximate the rounding noise. This has been a massively successful approach; pretty much every paper on compression uses this method. However, this leads to a mismatch between training and test time performance.\nIn terms of novelty, this paper uses the posterior to encode the sample into bits, inspired by arithmetic coding.\nUniversal quantisation In this paper , a universal quantisation scheme is proposed where the problem of communicating a sample \\(z\\) from a noisy channel \\(p(z|x)\\) using as few bits as possible is considered. Theoretically, it is shown that the number of bits to communicate the sample \\(z\\) is not much larger than the mutual information \\(I(x; z)\\) (see paper citation).\nThis quantisation technique is practically implemented in this paper , called relative entropy coding (REC), where an image compression problem is considered using this technique. Relative entropy coding can be defined as follows\nDefinition (Relative entropy coding): Let Q be a target and P be a proposal distribution, with \\(D_{KL}(Q || P) \u0026lt; \\infty\\), and let \\(S = (s_1,s_2, \u0026hellip;)\\) be an infinite sequence of publicly available independent fair coin tosses. Relative entropy coding is the problem of producing a uniquely decodable code \\(C\\) representing a sample from \\(Q\\) given \\(S\\), such that the codelength \\(|C|\\) satisfies \\( E[|C|] = \\mathcal{O}(D_{KL}(Q || P)) \\). The key observation is that, in the VAE setup used in that paper, there is a common random distribution, which is the prior of the VAE \\( p(z) \\). Therefore, the amount of information that needs to be sent is only characterised by the KL divergence between the posterior and the prior \\( D(p(z|x) || p(z)) \\). This is highly related to rate-distortion-perception as shown in this paper . The authors show that if it is required that perception (distribution) is perfectly matched, then in some settings, stochastic encoders/decoders with common randomness (e.g., universal quantisation), can achieve lower distortion than deterministic ones.\nAn interesting potential use case for universal quantisation is channel feedback. In a wireless communication channel, the posterior \\( p(z|x) \\) is determined by the channel. Then in order to losslessly send a sample \\( z \\sim p(z|x) \\) from the receiver to the transmitter, REC stipulates that at least \\( \\mathcal{O}(D_{KL}(Q || P)) \\) many bits need to be used. However, the problem is that the receiver only ever observes one sample from the posterior but the universal quantisation method samples from a prior rather than from the posterior. Instead, we can use the hybrid coding scheme, introduced in this paper , where instead of only sending the index, both the quantised sample and the index of the candidate are transmitted. The coding cost of the index is the same while there is an additional cost for transmitting the quantised sample. It is unclear whether this can give good results compared to conventional paradigm of quantising directly.\nA major problem of REC is that without any assumptions on \\(Q\\) and \\(P\\), the runtime of any REC algorithm is \\( \\mathcal{O}( \\exp( D_{\\infty}(Q||P) ) ) \\). However, if \\( dQ/dP \\) (the density ratio) is unimodal, then it can be done in \\( \\mathcal{O}( D_{\\infty}(Q||P) ) \\) (see here ) .\nThe key thing to note about REC is that because the indices have a distribution equal to the posterior, sampling the index is equivalent to sampling from the posterior distribution. Therefore, a code which uniquely identifies the index of a sample can recover the sample exactly. This is a little mind bending. It is somewhat counter intuitive that an integer can have a distribution which is the same as a continuous random variable. But I suppose the key is that the common random randomness is itself continuous, which affords the code to be discrete. Of course, there is also the bias, which never goes away but is negligible if the number of samples from the prior is large enough. The cost is that, for each sample transmitted, a large number of candidates need to be drawn, which is where the \\( \\mathcal{O}( \\exp( D_{\\infty}(Q||P) ) ) \\) runtime complexity comes from.\nAn important theorem that universal quantisation relies on is the strong functional representation (SFR) lemma, which states that for general \\( (X, Y) \\), there exists a \\( Z \\) that is independent of \\( X \\) and \\( Y \\) is a function of \\( (X, Z) \\), such that \\( H(Y | Z) \\leq I(X; Y) + \\log(I(X; Y) + 1) + 4 \\).\nFeedback It is known that feedback does not increase the capacity of a point-to-point channel but can increase reliability.\nPosterior matching Posterior matching is a generalised noiseless feedback scheme that is doubly exponentially capacity achieving for AWGN channels.\nTheorem (Optimal posterior matching): Let \\( \\theta \\) be the message point, \\( P_X \\) the channel input distribution, \\( P_{Y|X} \\) the channel likelihood, and \\( g_n \\) be the transmitter function at time step \\( n \\). Then the optimal posterior matching scheme is defined as \\( g_1(\\theta) = F^{-1}_X (\\theta) \\), \\( g_{n+1}(\\theta | y^n) = (F^{-1}_X \\circ F_{X | Y}(\\cdot | y_n)) \\circ g_n(\\theta | y^{n-1}) \\). The way to think about posterior matching is that the transmitter, observing the noiseless feedback of the receiver\u0026rsquo;s belief, is trying to send new information so that the receiver is steered to the correct answer.\nInterestingly, there are some similarities between posterior matching and universal quantisation . In posterior matching, the next transmitted random variable \\( X_{n+1} \\) is independent of the previous received values \\( Y^n \\), to ensure that it represents new information. Moreover, the message \\( \\theta \\) can be almost surely uniquely recovered from \\( (X_{n+1}, Y^n) \\) as \\( n \\rightarrow \\infty \\). Therefore, considering the SFR lemma, where it is shown that for general \\( (X, Y) \\), there exists a \\( Z \\) that is independent of \\( X \\), and \\( Y \\) is a function of \\( (X, Z) \\), such that \\( H(Y | Z) \\leq I(X; Y) + \\log(I(X; Y) + 1) + 4 \\). If we map \\( Z \\) to \\( X_{n+1} \\) in the previous problem and \\( Y \\mapsto \\theta \\), \\( X \\mapsto Y^n \\), then we have \\( H(\\theta | X_{n+1}) \\leq I(Y^n; \\theta) + \\log(I(Y^n; \\theta) + 1) + 4 \\). We can interpret \\( H(\\theta | X_{n+1}) \\) as any further uncertainty about the message after transmitting \\( X_{n+1} \\) and \\( I(Y^n; \\theta) \\) as knowledge the receiver has gained already.\n","permalink":"https://r0ymanesco.github.io/my-notes/posts/information_theory/","summary":"Rate-distortion This is fairly straight forward, will complete later.\nUniversal coding Universal coding is concerned with meeting a distortion constraint when the source distribution \\(p\\) is unknown. A coding scheme is said to be universal over a class of source distributions if the rate redundancy converges to zero for every source in that class.\nd-semifaithful coding \\(d\\)-semifaithful coding is a form of lossy compression in which the decoder outputs a reconstruction sequence that is within distortion \\(d\\) of the original source sequence with probability 1.","title":"Information Theory"},{"content":"Potential customer contacts Qualcomm Arash Behboodi (ai research) Dashan Gao (machine learning, ai) Tharak Krishnan (product and engineer leader) Yasutomo Matsuba (principle engineer video) Apple Said Medjkouh (RnD, wireless tech/ecosystem) Alan Carlton (wireless research) Kenza Hamidouche (wireless systems architect) Krisztian Kiss (senior wireless standards) Onur Sahin (wireless research) FB Joel Kyungho Kim (head of cellular tech) Jean-Yves Bouguet (head of ar/vr) BT Ed Hunter (Cellular for Drones \u0026amp; XCelerate Future Flight Project Manager) Intel Gila Kamhi (chief ai officer) Oner Orhan (AI/ML Research Scientist at Intel Labs) Jodie Frew (IoT, ai, business acceleration) Imagination Tim Atherton (director of ai) Arm Sofiane Yous (Senior Principal Embedded AI/ML) Funding tips from Alexandra There are basically 3 stages between idea to series A:\nPre-seed Seed Series A In each round, expect to lose 15-25% of equity (25% is very bad). By the time you reach series A, you need to not only be making revenue (not necessarily profit), but also hold 50% of the company between the people that work at the company full time.\nThere are tax benefits in the UK for individuals that invest in startups ( SEIS , EIS ), which is why angel investors will do this rather than invest in other things (e.g., stocks). In other to be elegible for those UK tax benefits, you need to be an incorporated company.\nA good way to calculate the ownership ratio is to consider how many hours each founder spends on the company each week.\nSmart home Challenges Security This is the same as Ana-Maria/Hamed\u0026rsquo;s startup. This is assumed to be handled in the application layer.\nConnectivity/reliability This is where we might be able to innovate. Typical smart home protocol uses 802.14.5 as the PHY standard. It doesn\u0026rsquo;t specify any channel codes, only a DSSS is used. If we can come up with a very simple channel code that can run in software, then this might improve the reliability of smart home devices. Any solution needs to be very easy to implement (no new hardware requirements).\nFrom looking at the Thread API, it seems that the main challenges worth addressing are:\nLow data rate, short rage This is currently solved using a mesh network, which is a multi-hop problem. An implementation of channel coding could also help but would have to be extremely efficient. Recall the key to this space is energy efficiency. JSCC could certain help if we can treat the whole mesh as a single systme and do some kind of task oriented coding.\nTopological reconfiguration after node dropout Thread implements self-healing, which is when a single device drops out of the mesh, it can reconfigure its topology without it. It is not clear yet whether this reconfiguration takes into account the network environment. If it doesn\u0026rsquo;t, this can have a big impact on performance if the topology can be reconfigured to ensure reliability/latency due to channel quality.\nBandwidth management This is due to the large number of devices that could be on the network. It is not clear how they are managing this either, possibly not a lot of optimization into this. This problem can be folded into the data rate/range problem.\nIn fact, these three problems can all be folded into one optimization problem. OpenThread also offers open source APIs to test new algorithms.\nMarket segmentation/interoperability Is it possible for someone to come up with a universal API for all smart home standards? I think this will be increasingly irrelevant as more manufacturers start using Thread.\nMultiple-access relay channel (MARC) A smarthome network is basically a mutiple-access relay channel (MARC).\nIntegrated sensing and communications (ISAC) ISAC for multiple access channels (MAC) is also amiable to JSCC. This is mostly due to the fact that MACs can gain more significantly than point-to-point.\nCustomer discovery Skydio The meeting verified our current solution cannot go to market but a discrete outter JSCC code using existing PHY implementation could. It also verified that targetting markets that have solutions that are very good already will not succeed. The issue is in order to just close the gap in hardware compared to existing implementations, the capital cost is already too high. We need to target problems that don\u0026rsquo;t have good solutions where the gains can be obtained without closing those gaps.\n","permalink":"https://r0ymanesco.github.io/my-notes/posts/waiveform/","summary":"Potential customer contacts Qualcomm Arash Behboodi (ai research) Dashan Gao (machine learning, ai) Tharak Krishnan (product and engineer leader) Yasutomo Matsuba (principle engineer video) Apple Said Medjkouh (RnD, wireless tech/ecosystem) Alan Carlton (wireless research) Kenza Hamidouche (wireless systems architect) Krisztian Kiss (senior wireless standards) Onur Sahin (wireless research) FB Joel Kyungho Kim (head of cellular tech) Jean-Yves Bouguet (head of ar/vr) BT Ed Hunter (Cellular for Drones \u0026amp; XCelerate Future Flight Project Manager) Intel Gila Kamhi (chief ai officer) Oner Orhan (AI/ML Research Scientist at Intel Labs) Jodie Frew (IoT, ai, business acceleration) Imagination Tim Atherton (director of ai) Arm Sofiane Yous (Senior Principal Embedded AI/ML) Funding tips from Alexandra There are basically 3 stages between idea to series A:","title":"WAIveform"}]